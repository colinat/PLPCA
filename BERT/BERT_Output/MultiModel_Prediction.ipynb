{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#n_gpu = torch.cuda.device_count()\n",
    "#torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nskPzUM084zL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "output_dir = './model_save/'\n",
    "s_output_dir = './s_model_save/'\n",
    "e_output_dir = './e_model_save/'\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir, num_labels = 11)\n",
    "s_model = BertForSequenceClassification.from_pretrained(s_output_dir, num_labels = 3)\n",
    "e_model = BertForSequenceClassification.from_pretrained(e_output_dir, num_labels = 4)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "s_model.to(device)\n",
    "e_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Xoy5f3baPXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the model via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aoROEEJBN5bE"
   },
   "outputs": [],
   "source": [
    "file_path = \"Western Union Co_20170502-Text.txt\"\n",
    "file_name = os.path.basename(file_path)\n",
    "file_name = os.path.splitext(file_name)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7cpjyGni2_O"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hba10sXR7Xi6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 205 transcripts\n",
      "Tokenizing Transcripts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:00<00:00, 106612.81it/s]\n",
      "213it [00:00, 25570.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented 213 transcript sentences\n",
      "Done writing to corpus_Western Union Co_20170502-Text.txt\n",
      "Max sentence length: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 213 test sentences...\n",
      "Prediction took: 0:01:04\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "scripts = []\n",
    "with open(file_path, 'r') as file:\n",
    "  mydata = file.readlines()\n",
    "  for lines in mydata:\n",
    "    scripts.append(lines)\n",
    "\n",
    "# get sentence segemented review with #sentences > 2\n",
    "def sentence_segment_filter_docs(doc_array):\n",
    "    sentences = []\n",
    "    for doc in nlp.pipe(doc_array, disable=['parser', 'tagger', 'ner'], batch_size=1000, n_threads=8):\n",
    "        sentences.append([sent.text.strip() for sent in doc.sents])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "print(f'Found {len(scripts)} transcripts')\n",
    "print(f'Tokenizing Transcripts...')\n",
    "\n",
    "sentences = sentence_segment_filter_docs(scripts)\n",
    "nr_sents = sum([len(s) for s in sentences])\n",
    "print(f'Segmented {nr_sents} transcript sentences')\n",
    "\n",
    "\n",
    "sentences = sentence_segment_filter_docs(scripts)\n",
    "\n",
    "# Save to file\n",
    "fn_out = f'corpus_{file_name}.txt'\n",
    "\n",
    "with open(fn_out, \"w\") as f:\n",
    "    for sents in tqdm(sentences):\n",
    "        real_sents = []\n",
    "        for s in sents:\n",
    "            x = s.replace(' ', '').replace('\\n', '')\n",
    "            if x != '':\n",
    "                real_sents.append(s.replace('\\n', ''))\n",
    "        # filter only paragraph more than or equal to 1 sentence        \n",
    "        if len(real_sents) >= 1:\n",
    "            str_to_write = \"\\n\".join(real_sents) + \"|||\" + \"\\n\"\n",
    "            f.write(str_to_write)\n",
    "\n",
    "print(f'Done writing to {fn_out}')\n",
    "\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "\n",
    "    def __init__(self, unique_id, text_a, text_b):\n",
    "        self.unique_id = unique_id\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "\n",
    "corpus = []\n",
    "unique_id = 0\n",
    "count = []\n",
    "with open(fn_out, \"r\", encoding='utf-8') as input_file:\n",
    "  for line in tqdm(input_file):\n",
    "    line = line.strip()\n",
    "    text_a = None\n",
    "    text_b = None\n",
    "    m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "    if m is None:\n",
    "      text_a = re.sub(r\"(\\|\\|\\|)$\", \"\", line)\n",
    "    else:\n",
    "      text_a = m.group(1)\n",
    "      text_b = m.group(2)\n",
    "    corpus.append(InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "    unique_id += 1\n",
    "    cnt = len(line.split())\n",
    "    count.append(cnt)\n",
    "\n",
    "MAX_LEN = int(math.ceil(max(count)/10)*10)\n",
    "\n",
    "print('Max sentence length: ' + str(MAX_LEN))\n",
    "\n",
    "# Set the maximum sequence length.\n",
    "# In the original paper, the authors used a length of 512.\n",
    "seq_length = MAX_LEN \n",
    "# type=int\n",
    "# The maximum total input sequence length after WordPiece tokenization. \n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "\n",
    "features = []\n",
    "for (txt_index, sent_pair) in enumerate(corpus):\n",
    "    tokens_a = tokenizer.tokenize(sent_pair.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if sent_pair.text_b:\n",
    "        tokens_b = tokenizer.tokenize(sent_pair.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        input_type_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "        \n",
    "    features.append(InputFeatures(\n",
    "                unique_id=sent_pair.unique_id,\n",
    "                tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                input_type_ids=input_type_ids))\n",
    "    \n",
    "# For Prediction, we try higher batch size of 32\n",
    "\n",
    "batch_size = 32\n",
    "local_rank = -1 \n",
    "#local_rank for distributed training on gpus\n",
    "\n",
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model.\n",
    "unique_id_to_feature = {}\n",
    "for feature in features:\n",
    "    unique_id_to_feature[feature.unique_id] = feature\n",
    "\n",
    "#if local_rank != -1:\n",
    "    #model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "#elif n_gpu > 1:\n",
    "    #model = torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    "# Convert to tensors, need \"input_ids & its index\", \"input_mask\" and \"input_label\"\n",
    "# For testing set\n",
    "prediction_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long) # Token ids for every sentences in individual list\n",
    "prediction_input_ids_index = torch.arange(prediction_input_ids.size(0), dtype=torch.long) # Index for each sentences in one list\n",
    "prediction_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "\n",
    "prediction_data = TensorDataset(prediction_input_ids, prediction_input_mask, prediction_input_ids_index)\n",
    "\n",
    "# Create the DataLoader for our testing set.\n",
    "if local_rank == -1:\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "else:\n",
    "    prediction_sampler = DistributedSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size) \n",
    "# No of item in dataloader = Total sample / Batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "s_model.eval()\n",
    "e_model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions = None\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU/CPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_input_ids_index = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "      s_logits = s_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "      e_logits = e_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "      \n",
    "      y_prob = logits[0].softmax(dim = -1)  \n",
    "      s_y_prob = s_logits[0].softmax(dim = -1)\n",
    "      e_y_prob = e_logits[0].softmax(dim = -1)\n",
    "\n",
    "      if predictions is None:\n",
    "        predictions = y_prob.detach().cpu().numpy()\n",
    "        a_class = np.argmax(predictions, axis=1).flatten()\n",
    "        \n",
    "        s_predictions = s_y_prob.detach().cpu().numpy()\n",
    "        s_class = np.argmax(s_predictions, axis=1).flatten()\n",
    "        \n",
    "        e_predictions = e_y_prob.detach().cpu().numpy()\n",
    "        e_class = np.argmax(e_predictions, axis=1).flatten()\n",
    "\n",
    "      else:\n",
    "        predictions = np.concatenate((predictions, y_prob.detach().cpu().numpy()), axis=0)\n",
    "        a_class = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "        s_predictions = np.concatenate((s_predictions, s_y_prob.detach().cpu().numpy()), axis=0)\n",
    "        s_class = np.argmax(s_predictions, axis=1).flatten()\n",
    "        \n",
    "        e_predictions = np.concatenate((e_predictions, e_y_prob.detach().cpu().numpy()), axis=0)\n",
    "        e_class = np.argmax(e_predictions, axis=1).flatten()\n",
    "        \n",
    "        \n",
    "        \n",
    "# Categories of Aspects\n",
    "label_list = [\"sales\",\"earnings\",\"op_costs\",\"products_services\",\"organic_expansion\",\"acquisitions\",\"competition\",\"op_risks\",\"debt\",\"not_applicable\",\"NIL\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "num_labels = len(label_list) # 11\n",
    "\n",
    "# Categories of Sentiment\n",
    "Slabel_list = [\"Negative\",\"Neutral\",\"Positive\",] # Follow order in Slabel_f\n",
    "Slabel2id = {label: i for i, label in enumerate(Slabel_list)}\n",
    "Sid2label = {i: label for i, label in enumerate(Slabel_list)}\n",
    "s_num_labels = len(Slabel_list) # 3\n",
    "\n",
    "# Categories of Emotion\n",
    "Elabel_list = [\"Confident\",\"Dodgy\",\"NIL\",\"Uncertain\"] # Follow order in Elabel_f\n",
    "Elabel2id = {label: i for i, label in enumerate(Elabel_list)}\n",
    "Eid2label = {i: label for i, label in enumerate(Elabel_list)}\n",
    "e_num_labels = len(Elabel_list) # 4\n",
    "\n",
    "# Extract the text used by the tokenizer\n",
    "flat_txt = []\n",
    "for (txt_index, sent_pair) in enumerate(corpus):\n",
    "  txt = sent_pair.text_a\n",
    "  flat_txt.append(txt)\n",
    "\n",
    "# Concat the ids to the Aspect, Sentiments and Emotion lables\n",
    "a_txt = [str(a)+ \"-\" + id2label[int(a)] for a in np.nditer(a_class)]\n",
    "s_txt = [str(s)+ \"-\" + Sid2label[int(s)] for s in np.nditer(s_class)]\n",
    "e_txt = [str(e)+ \"-\" + Eid2label[int(e)] for e in np.nditer(e_class)]\n",
    "\n",
    "# Concat the Predictions to a dataframe\n",
    "text_df = pd.DataFrame(data=flat_txt, columns = [\"text\"])\n",
    "a_class_df = pd.DataFrame(data=a_txt, columns = [\"Aspect\"])\n",
    "s_class_df = pd.DataFrame(data=s_txt, columns = [\"Sentiment\"])\n",
    "e_class_df = pd.DataFrame(data=e_txt, columns = [\"Emotion\"])\n",
    "\n",
    "\n",
    "a_df = pd.DataFrame(data=predictions, columns = list(label2id))\n",
    "\n",
    "s_df = pd.DataFrame(data=s_predictions, columns = list(Slabel2id))\n",
    "\n",
    "e_df = pd.DataFrame(data=e_predictions, columns = list(Elabel2id))\n",
    "\n",
    "output_df = pd.concat([text_df, a_class_df, s_class_df, e_class_df, a_df, s_df, e_df], axis=1)\n",
    "\n",
    "# Saving to CSV\n",
    "output_df.to_csv('predicted1.csv', index=True, header=True)\n",
    "    \n",
    "    \n",
    "print(\"Prediction took: {:}\".format(format_time(time.time() - t0)))  \n",
    "print('    DONE.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IOEYYuMkols"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Aspect Classification across 11 Aspects is 0.9989999756217003\n",
      "Sum of Sentiment Classification Total Probability across 3 Sentiment is 1.0000000050058588\n",
      "Sum of Emotion Classification Total Probability across 4 Emotion is 1.0000000222353265\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>sales</th>\n",
       "      <th>earnings</th>\n",
       "      <th>op_costs</th>\n",
       "      <th>products_services</th>\n",
       "      <th>organic_expansion</th>\n",
       "      <th>acquisitions</th>\n",
       "      <th>...</th>\n",
       "      <th>debt</th>\n",
       "      <th>not_applicable</th>\n",
       "      <th>NIL</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Confident</th>\n",
       "      <th>Dodgy</th>\n",
       "      <th>NIL</th>\n",
       "      <th>Uncertain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Business Solutions revenues declined 6% or 3% ...</td>\n",
       "      <td>1-earnings</td>\n",
       "      <td>0-Negative</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.048653</td>\n",
       "      <td>0.908751</td>\n",
       "      <td>0.013110</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>0.007603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.970712</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>0.026494</td>\n",
       "      <td>0.041203</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.947248</td>\n",
       "      <td>0.010663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>We can be even better there, having the right ...</td>\n",
       "      <td>1-earnings</td>\n",
       "      <td>2-Positive</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.024880</td>\n",
       "      <td>0.333908</td>\n",
       "      <td>0.287877</td>\n",
       "      <td>0.028933</td>\n",
       "      <td>0.040301</td>\n",
       "      <td>0.003152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>0.021870</td>\n",
       "      <td>0.200412</td>\n",
       "      <td>0.018283</td>\n",
       "      <td>0.317467</td>\n",
       "      <td>0.664250</td>\n",
       "      <td>0.370206</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>0.604078</td>\n",
       "      <td>0.023307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Our customers really don't have to leave the m...</td>\n",
       "      <td>3-products_services</td>\n",
       "      <td>1-Neutral</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.048526</td>\n",
       "      <td>0.031799</td>\n",
       "      <td>0.011980</td>\n",
       "      <td>0.388563</td>\n",
       "      <td>0.026304</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.197168</td>\n",
       "      <td>0.266475</td>\n",
       "      <td>0.013992</td>\n",
       "      <td>0.931684</td>\n",
       "      <td>0.054325</td>\n",
       "      <td>0.115261</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>0.833769</td>\n",
       "      <td>0.045629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>We recorded $14 million of WU Way expenses in ...</td>\n",
       "      <td>2-op_costs</td>\n",
       "      <td>2-Positive</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>0.056989</td>\n",
       "      <td>0.840234</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.026417</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012039</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.024528</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>0.073355</td>\n",
       "      <td>0.915441</td>\n",
       "      <td>0.277299</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>0.690457</td>\n",
       "      <td>0.029290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>the infrastructure is right to improve there</td>\n",
       "      <td>9-not_applicable</td>\n",
       "      <td>1-Neutral</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.936190</td>\n",
       "      <td>0.056970</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.989476</td>\n",
       "      <td>0.009699</td>\n",
       "      <td>0.022568</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.975968</td>\n",
       "      <td>0.001132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>In addition, we have changed the methodology o...</td>\n",
       "      <td>0-sales</td>\n",
       "      <td>1-Neutral</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.894372</td>\n",
       "      <td>0.083060</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.083388</td>\n",
       "      <td>0.463083</td>\n",
       "      <td>0.453529</td>\n",
       "      <td>0.112637</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.839491</td>\n",
       "      <td>0.044071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>You know our business very well, Ashwin, depen...</td>\n",
       "      <td>9-not_applicable</td>\n",
       "      <td>1-Neutral</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.014367</td>\n",
       "      <td>0.017743</td>\n",
       "      <td>0.012068</td>\n",
       "      <td>0.015073</td>\n",
       "      <td>0.015121</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.465110</td>\n",
       "      <td>0.437931</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>0.983937</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.211924</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.737437</td>\n",
       "      <td>0.044312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>The higher tax rate in the quarter was due to ...</td>\n",
       "      <td>10-NIL</td>\n",
       "      <td>0-Negative</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.094916</td>\n",
       "      <td>0.156458</td>\n",
       "      <td>0.072805</td>\n",
       "      <td>0.144568</td>\n",
       "      <td>0.112398</td>\n",
       "      <td>0.019536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>0.095582</td>\n",
       "      <td>0.233808</td>\n",
       "      <td>0.636758</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.083672</td>\n",
       "      <td>0.087676</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.896938</td>\n",
       "      <td>0.013262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>That's something that we are extremely optimis...</td>\n",
       "      <td>0-sales</td>\n",
       "      <td>2-Positive</td>\n",
       "      <td>0-Confident</td>\n",
       "      <td>0.435926</td>\n",
       "      <td>0.044272</td>\n",
       "      <td>0.011690</td>\n",
       "      <td>0.022795</td>\n",
       "      <td>0.181553</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.057259</td>\n",
       "      <td>0.211716</td>\n",
       "      <td>0.008604</td>\n",
       "      <td>0.107139</td>\n",
       "      <td>0.884257</td>\n",
       "      <td>0.546111</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.415448</td>\n",
       "      <td>0.034570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Revenue growth in the region is consistent wit...</td>\n",
       "      <td>0-sales</td>\n",
       "      <td>2-Positive</td>\n",
       "      <td>0-Confident</td>\n",
       "      <td>0.956939</td>\n",
       "      <td>0.024689</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.005211</td>\n",
       "      <td>0.006709</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.960743</td>\n",
       "      <td>0.962446</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.033721</td>\n",
       "      <td>0.002964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text               Aspect  \\\n",
       "43   Business Solutions revenues declined 6% or 3% ...           1-earnings   \n",
       "166  We can be even better there, having the right ...           1-earnings   \n",
       "185  Our customers really don't have to leave the m...  3-products_services   \n",
       "54   We recorded $14 million of WU Way expenses in ...           2-op_costs   \n",
       "111       the infrastructure is right to improve there     9-not_applicable   \n",
       "17   In addition, we have changed the methodology o...              0-sales   \n",
       "158  You know our business very well, Ashwin, depen...     9-not_applicable   \n",
       "60   The higher tax rate in the quarter was due to ...               10-NIL   \n",
       "209  That's something that we are extremely optimis...              0-sales   \n",
       "28   Revenue growth in the region is consistent wit...              0-sales   \n",
       "\n",
       "      Sentiment      Emotion     sales  earnings  op_costs  products_services  \\\n",
       "43   0-Negative        2-NIL  0.048653  0.908751  0.013110           0.006053   \n",
       "166  2-Positive        2-NIL  0.024880  0.333908  0.287877           0.028933   \n",
       "185   1-Neutral        2-NIL  0.048526  0.031799  0.011980           0.388563   \n",
       "54   2-Positive        2-NIL  0.001914  0.056989  0.840234           0.005711   \n",
       "111   1-Neutral        2-NIL  0.000741  0.000707  0.000727           0.001918   \n",
       "17    1-Neutral        2-NIL  0.894372  0.083060  0.001807           0.007982   \n",
       "158   1-Neutral        2-NIL  0.014367  0.017743  0.012068           0.015073   \n",
       "60   0-Negative        2-NIL  0.094916  0.156458  0.072805           0.144568   \n",
       "209  2-Positive  0-Confident  0.435926  0.044272  0.011690           0.022795   \n",
       "28   2-Positive  0-Confident  0.956939  0.024689  0.000817           0.005211   \n",
       "\n",
       "     organic_expansion  acquisitions  ...      debt  not_applicable       NIL  \\\n",
       "43            0.006235      0.007603  ...  0.002613        0.000572  0.001907   \n",
       "166           0.040301      0.003152  ...  0.009348        0.021870  0.200412   \n",
       "185           0.026304      0.000999  ...  0.001780        0.197168  0.266475   \n",
       "54            0.026417      0.005680  ...  0.012039        0.002256  0.024528   \n",
       "111           0.000847      0.000090  ...  0.000203        0.936190  0.056970   \n",
       "17            0.004843      0.001023  ...  0.000488        0.001400  0.002108   \n",
       "158           0.015121      0.000321  ...  0.001991        0.465110  0.437931   \n",
       "60            0.112398      0.019536  ...  0.012565        0.095582  0.233808   \n",
       "209           0.181553      0.001500  ...  0.005102        0.057259  0.211716   \n",
       "28            0.006709      0.000690  ...  0.000339        0.000954  0.001473   \n",
       "\n",
       "     Negative   Neutral  Positive  Confident     Dodgy       NIL  Uncertain  \n",
       "43   0.970712  0.002794  0.026494   0.041203  0.000886  0.947248   0.010663  \n",
       "166  0.018283  0.317467  0.664250   0.370206  0.002409  0.604078   0.023307  \n",
       "185  0.013992  0.931684  0.054325   0.115261  0.005340  0.833769   0.045629  \n",
       "54   0.011204  0.073355  0.915441   0.277299  0.002954  0.690457   0.029290  \n",
       "111  0.000825  0.989476  0.009699   0.022568  0.000331  0.975968   0.001132  \n",
       "17   0.083388  0.463083  0.453529   0.112637  0.003802  0.839491   0.044071  \n",
       "158  0.002495  0.983937  0.013568   0.211924  0.006327  0.737437   0.044312  \n",
       "60   0.636758  0.279570  0.083672   0.087676  0.002124  0.896938   0.013262  \n",
       "209  0.008604  0.107139  0.884257   0.546111  0.003871  0.415448   0.034570  \n",
       "28   0.024684  0.014574  0.960743   0.962446  0.000869  0.033721   0.002964  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.round(predictions, decimals=3)\n",
    "print(\"No. of Aspect Classification across \" + str(len(predictions[0])) + \" Aspects is \" + str(sum(predictions[0])))\n",
    "\n",
    "s_predictions = np.round(s_predictions, decimals=3)\n",
    "print(\"Sum of Sentiment Classification Total Probability across \" + str(len(s_predictions[0])) + \" Sentiment is \" + str(sum(s_predictions[0])))\n",
    "\n",
    "e_predictions = np.round(e_predictions, decimals=3)\n",
    "print(\"Sum of Emotion Classification Total Probability across \" + str(len(e_predictions[0])) + \" Emotion is \" + str(sum(e_predictions[0])))\n",
    "\n",
    "output_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Multitask_Prediction_BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "451px",
    "left": "909px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
