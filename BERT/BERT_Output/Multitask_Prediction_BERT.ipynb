{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14CQo7-RAKRC"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.modeling_bert import BertPreTrainedModel, BertModel\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "class BertForMultitask(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForMultitask, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        self.classifier = nn.Linear(config.hidden_size, out_features=11)\n",
    "        self.s_classifier = nn.Linear(config.hidden_size, out_features=3)\n",
    "        self.e_classifier = nn.Linear(config.hidden_size, out_features=4)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None, \n",
    "                labels=None, s_labels=None, e_labels=None):\n",
    "        _, pooled_output  = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        s_logits = self.s_classifier(pooled_output)\n",
    "        e_logits = self.e_classifier(pooled_output)\n",
    "        \n",
    "        outputs = logits, s_logits, e_logits\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nskPzUM084zL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultitask(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       "  (s_classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (e_classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#output_dir = './drive/My Drive/EBAC_G/NLP_Project/BERT/model_Multitask/'\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForMultitask.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Xoy5f3baPXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the model via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aoROEEJBN5bE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 205 transcripts\n",
      "Tokenizing Transcripts...\n",
      "Segmented 213 transcript sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:00<00:00, 95388.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing to corpus_Western Union Co_20170502-Text.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Western Union Co_20170502-Text.txt\"\n",
    "file_name = os.path.basename(file_path)\n",
    "file_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "scripts = []\n",
    "with open(file_path, 'r') as file:\n",
    "  mydata = file.readlines()\n",
    "  for lines in mydata:\n",
    "    scripts.append(lines)\n",
    "\n",
    "# get sentence segemented review with #sentences > 2\n",
    "def sentence_segment_filter_docs(doc_array):\n",
    "    sentences = []\n",
    "    for doc in nlp.pipe(doc_array, disable=['parser', 'tagger', 'ner'], batch_size=1000, n_threads=8):\n",
    "        sentences.append([sent.text.strip() for sent in doc.sents])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "print(f'Found {len(scripts)} transcripts')\n",
    "print(f'Tokenizing Transcripts...')\n",
    "\n",
    "sentences = sentence_segment_filter_docs(scripts)\n",
    "nr_sents = sum([len(s) for s in sentences])\n",
    "print(f'Segmented {nr_sents} transcript sentences')\n",
    "\n",
    "\n",
    "sentences = sentence_segment_filter_docs(scripts)\n",
    "\n",
    "# Save to file\n",
    "fn_out = f'corpus_{file_name}.txt'\n",
    "\n",
    "with open(fn_out, \"w\") as f:\n",
    "    for sents in tqdm(sentences):\n",
    "        real_sents = []\n",
    "        for s in sents:\n",
    "            x = s.replace(' ', '').replace('\\n', '')\n",
    "            if x != '':\n",
    "                real_sents.append(s.replace('\\n', ''))\n",
    "        # filter only paragraph more than or equal to 1 sentence        \n",
    "        if len(real_sents) >= 1:\n",
    "            str_to_write = \"\\n\".join(real_sents) + \"|||\" + \"\\n\"\n",
    "            f.write(str_to_write)\n",
    "\n",
    "print(f'Done writing to {fn_out}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7cpjyGni2_O"
   },
   "source": [
    "### Preprocessing the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0q7Y3vog7wM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "213it [00:00, 26869.58it/s]\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   ******\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   unique_id: 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   tokens: [CLS] thank you , hi ##km ##et [SEP]\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_ids: 101 4067 2017 1010 7632 22287 3388 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   ******\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   unique_id: 1\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   tokens: [CLS] first quarter reported revenues of $ 1 . 3 billion were flat or increased 3 % on a constant currency basis compared to the prior - year period [SEP]\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_ids: 101 2034 4284 2988 12594 1997 1002 1015 1012 1017 4551 2020 4257 2030 3445 1017 1003 2006 1037 5377 9598 3978 4102 2000 1996 3188 1011 2095 2558 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   ******\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   unique_id: 2\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   tokens: [CLS] the impact of currency translation , net of hedge benefits , reduced first quarter revenue by approximately $ 30 million compared to the prior year [SEP]\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_ids: 101 1996 4254 1997 9598 5449 1010 5658 1997 17834 6666 1010 4359 2034 4284 6599 2011 3155 1002 2382 2454 4102 2000 1996 3188 2095 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   ******\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   unique_id: 3\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   tokens: [CLS] in the consumer - to - consumer segment , revenues were flat in the quarter or increased 2 % constant currency , while transactions grew 2 % [SEP]\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_ids: 101 1999 1996 7325 1011 2000 1011 7325 6903 1010 12594 2020 4257 1999 1996 4284 2030 3445 1016 1003 5377 9598 1010 2096 11817 3473 1016 1003 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   ******\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   unique_id: 4\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   tokens: [CLS] c2 ##c constant currency revenue benefited from strong growth in western ##uni ##on [SEP]\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_ids: 101 29248 2278 5377 9598 6599 19727 2013 2844 3930 1999 2530 19496 2239 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/23/2020 19:31:06 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Max sentence length: 50\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class InputExample(object):\n",
    "\n",
    "    def __init__(self, unique_id, text_a, text_b):\n",
    "        self.unique_id = unique_id\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "\n",
    "corpus = []\n",
    "unique_id = 0\n",
    "count = []\n",
    "with open(fn_out, \"r\", encoding='utf-8') as input_file:\n",
    "  for line in tqdm(input_file):\n",
    "    line = line.strip()\n",
    "    text_a = None\n",
    "    text_b = None\n",
    "    m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "    if m is None:\n",
    "      text_a = re.sub(r\"(\\|\\|\\|)$\", \"\", line)\n",
    "    else:\n",
    "      text_a = m.group(1)\n",
    "      text_b = m.group(2)\n",
    "    corpus.append(InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "    unique_id += 1\n",
    "    cnt = len(line.split())\n",
    "    count.append(cnt)\n",
    "\n",
    "MAX_LEN = int(math.ceil(max(count)/10)*10)\n",
    "print(' ')\n",
    "print('Max sentence length: ' + str(MAX_LEN))\n",
    "\n",
    "# Set the maximum sequence length.\n",
    "# In the original paper, the authors used a length of 512.\n",
    "seq_length = MAX_LEN \n",
    "# type=int\n",
    "# The maximum total input sequence length after WordPiece tokenization. \n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "\n",
    "features = []\n",
    "for (txt_index, sent_pair) in enumerate(corpus):\n",
    "    tokens_a = tokenizer.tokenize(sent_pair.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if sent_pair.text_b:\n",
    "        tokens_b = tokenizer.tokenize(sent_pair.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        input_type_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "\n",
    "    if txt_index < 5:\n",
    "        logger.info(\"******\")\n",
    "        logger.info(\"unique_id: %s\" % (sent_pair.unique_id))\n",
    "        logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "        logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        logger.info(\"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "    \n",
    "        \n",
    "    features.append(InputFeatures(\n",
    "                unique_id=sent_pair.unique_id,\n",
    "                tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                input_type_ids=input_type_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUmsUOIv8EUO"
   },
   "source": [
    "## Making Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQjig_Wrg9X-"
   },
   "outputs": [],
   "source": [
    "# For Prediction, we try higher batch size of 32\n",
    "\n",
    "batch_size = 32\n",
    "local_rank = -1 \n",
    "#local_rank for distributed training on gpus\n",
    "\n",
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model.\n",
    "unique_id_to_feature = {}\n",
    "for feature in features:\n",
    "    unique_id_to_feature[feature.unique_id] = feature\n",
    "\n",
    "#if local_rank != -1:\n",
    "    #model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "#elif n_gpu > 1:\n",
    "    #model = torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    "# Convert to tensors, need \"input_ids & its index\", \"input_mask\" and \"input_label\"\n",
    "# For testing set\n",
    "prediction_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long) # Token ids for every sentences in individual list\n",
    "prediction_input_ids_index = torch.arange(prediction_input_ids.size(0), dtype=torch.long) # Index for each sentences in one list\n",
    "prediction_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "\n",
    "prediction_data = TensorDataset(prediction_input_ids, prediction_input_mask, prediction_input_ids_index)\n",
    "\n",
    "# Create the DataLoader for our testing set.\n",
    "if local_rank == -1:\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "else:\n",
    "    prediction_sampler = DistributedSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size) \n",
    "# No of item in dataloader = Total sample / Batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hba10sXR7Xi6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 213 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions = None\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_input_ids_index = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits, s_logits, e_logits = model(b_input_ids, token_type_ids=None, \n",
    "                                         attention_mask=b_input_mask)\n",
    "\n",
    "      y_prob = logits.softmax(dim = -1) # normalizes values along axis 1\n",
    "      s_y_prob = s_logits.softmax(dim = -1)\n",
    "      e_y_prob = e_logits.softmax(dim = -1)\n",
    "\n",
    "      if predictions is None:\n",
    "        predictions = y_prob.detach().cpu().numpy()\n",
    "        \n",
    "        s_predictions = s_y_prob.detach().cpu().numpy()\n",
    "        s_class = np.argmax(s_predictions, axis=1).flatten()\n",
    "        \n",
    "        e_predictions = e_y_prob.detach().cpu().numpy()\n",
    "        e_class = np.argmax(e_predictions, axis=1).flatten()\n",
    "\n",
    "      else:\n",
    "        predictions = np.concatenate((predictions, y_prob.detach().cpu().numpy()), axis=0)\n",
    "\n",
    "        s_predictions = np.concatenate((s_predictions, s_y_prob.detach().cpu().numpy()), axis=0)\n",
    "        s_class = np.argmax(s_predictions, axis=1).flatten()\n",
    "        \n",
    "        e_predictions = np.concatenate((e_predictions, e_y_prob.detach().cpu().numpy()), axis=0)\n",
    "        e_class = np.argmax(e_predictions, axis=1).flatten()\n",
    "  \n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IOEYYuMkols"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Aspect Mining Total Probability across 11 Aspects is 0.9990000128746033\n",
      "Sum of Sentiment Classification Total Probability across 3 Sentiment is 1.0\n",
      "Sum of Emotion Classification Total Probability across 4 Emotion is 0.9999999790452421\n"
     ]
    }
   ],
   "source": [
    "predictions = np.round(predictions, decimals=3)\n",
    "print(\"Sum of Aspect Mining Total Probability across \" + str(len(predictions[0])) + \" Aspects is \" + str(sum(predictions[0])))\n",
    "\n",
    "s_predictions = np.round(s_predictions, decimals=3)\n",
    "print(\"Sum of Sentiment Classification Total Probability across \" + str(len(s_predictions[0])) + \" Sentiment is \" + str(sum(s_predictions[0])))\n",
    "\n",
    "e_predictions = np.round(e_predictions, decimals=3)\n",
    "print(\"Sum of Emotion Classification Total Probability across \" + str(len(e_predictions[0])) + \" Emotion is \" + str(sum(e_predictions[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories of Aspects\n",
    "label_list = [\"sales\",\"earnings\",\"op_costs\",\"products_services\",\"organic_expansion\",\"acquisitions\",\"competition\",\"op_risks\",\"debt\",\"not_applicable\",\"NIL\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "num_labels = len(label_list) # 11\n",
    "\n",
    "# Categories of Sentiment\n",
    "Slabel_list = [\"Negative\",\"Neutral\",\"Positive\",] # Follow order in Slabel_f\n",
    "Slabel2id = {label: i for i, label in enumerate(Slabel_list)}\n",
    "Sid2label = {i: label for i, label in enumerate(Slabel_list)}\n",
    "s_num_labels = len(Slabel_list) # 3\n",
    "\n",
    "# Categories of Emotion\n",
    "Elabel_list = [\"Confident\",\"Dodgy\",\"NIL\",\"Uncertain\"] # Follow order in Elabel_f\n",
    "Elabel2id = {label: i for i, label in enumerate(Elabel_list)}\n",
    "Eid2label = {i: label for i, label in enumerate(Elabel_list)}\n",
    "e_num_labels = len(Elabel_list) # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6K4JCPRjJXZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>sales</th>\n",
       "      <th>earnings</th>\n",
       "      <th>op_costs</th>\n",
       "      <th>products_services</th>\n",
       "      <th>organic_expansion</th>\n",
       "      <th>acquisitions</th>\n",
       "      <th>competition</th>\n",
       "      <th>...</th>\n",
       "      <th>debt</th>\n",
       "      <th>not_applicable</th>\n",
       "      <th>NIL</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Confident</th>\n",
       "      <th>Dodgy</th>\n",
       "      <th>NIL</th>\n",
       "      <th>Uncertain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>But by the end of the year, we should be at ab...</td>\n",
       "      <td>1-Neutral</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Excluding these expenses, adjusted earnings pe...</td>\n",
       "      <td>0-Negative</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>And I know the team is doing a little – puttin...</td>\n",
       "      <td>2-Positive</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text   Sentiment Emotion  \\\n",
       "196  But by the end of the year, we should be at ab...   1-Neutral   2-NIL   \n",
       "63   Excluding these expenses, adjusted earnings pe...  0-Negative   2-NIL   \n",
       "156  And I know the team is doing a little – puttin...  2-Positive   2-NIL   \n",
       "\n",
       "     sales  earnings  op_costs  products_services  organic_expansion  \\\n",
       "196  0.005     0.083     0.028              0.021              0.013   \n",
       "63   0.064     0.694     0.155              0.010              0.009   \n",
       "156  0.034     0.073     0.063              0.395              0.062   \n",
       "\n",
       "     acquisitions  competition  ...   debt  not_applicable    NIL  Negative  \\\n",
       "196         0.005        0.003  ...  0.003           0.053  0.784     0.000   \n",
       "63          0.023        0.006  ...  0.007           0.009  0.012     0.995   \n",
       "156         0.014        0.014  ...  0.007           0.023  0.299     0.000   \n",
       "\n",
       "     Neutral  Positive  Confident  Dodgy    NIL  Uncertain  \n",
       "196    0.996     0.003      0.159  0.002  0.837      0.002  \n",
       "63     0.002     0.003      0.024  0.004  0.969      0.003  \n",
       "156    0.002     0.998      0.039  0.011  0.873      0.077  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the text used by the tokenizer\n",
    "flat_txt = []\n",
    "for (txt_index, sent_pair) in enumerate(corpus):\n",
    "  txt = sent_pair.text_a\n",
    "  flat_txt.append(txt)\n",
    "\n",
    "# Concat the ids to the Sentiments and Emotion lables\n",
    "s_txt = [str(s)+ \"-\" + Sid2label[int(s)] for s in np.nditer(s_class)]\n",
    "e_txt = [str(e)+ \"-\" + Eid2label[int(e)] for e in np.nditer(e_class)]\n",
    "\n",
    "# Concat the Predictions to a dataframe\n",
    "text_df = pd.DataFrame(data=flat_txt, columns = [\"text\"])\n",
    "s_class_df = pd.DataFrame(data=s_txt, columns = [\"Sentiment\"])\n",
    "e_class_df = pd.DataFrame(data=e_txt, columns = [\"Emotion\"])\n",
    "\n",
    "a_df = pd.DataFrame(data=predictions, columns = list(label2id))\n",
    "\n",
    "s_df = pd.DataFrame(data=s_predictions, columns = list(Slabel2id))\n",
    "\n",
    "e_df = pd.DataFrame(data=e_predictions, columns = list(Elabel2id))\n",
    "\n",
    "output_df = pd.concat([text_df, s_class_df, e_class_df, a_df, s_df, e_df], axis=1)\n",
    "\n",
    "# Saving to CSV\n",
    "pred_name = f'predicted_{file_name}.csv'\n",
    "output_df.to_csv(pred_name, index=True, header=True)\n",
    "\n",
    "output_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Multitask_Prediction_BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
