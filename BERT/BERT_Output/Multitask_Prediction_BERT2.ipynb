{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14CQo7-RAKRC"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.modeling_bert import BertPreTrainedModel, BertModel\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "class BertForMultitask(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForMultitask, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        self.classifier = nn.Linear(config.hidden_size, out_features=11)\n",
    "        self.s_classifier = nn.Linear(config.hidden_size, out_features=3)\n",
    "        self.e_classifier = nn.Linear(config.hidden_size, out_features=4)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None, \n",
    "                labels=None, s_labels=None, e_labels=None):\n",
    "        _, pooled_output  = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        s_logits = self.s_classifier(pooled_output)\n",
    "        e_logits = self.e_classifier(pooled_output)\n",
    "        \n",
    "        outputs = logits, s_logits, e_logits\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nskPzUM084zL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultitask(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       "  (s_classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (e_classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#output_dir = './drive/My Drive/EBAC_G/NLP_Project/BERT/model_Multitask/'\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForMultitask.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Xoy5f3baPXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aoROEEJBN5bE"
   },
   "outputs": [],
   "source": [
    "file_path = \"Western Union Co_20170502-Text.txt\"\n",
    "file_name = os.path.basename(file_path)\n",
    "file_name = os.path.splitext(file_name)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7cpjyGni2_O"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hba10sXR7Xi6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:00<00:00, 116999.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 205 transcripts\n",
      "Tokenizing Transcripts...\n",
      "Segmented 213 transcript sentences\n",
      "Done writing to corpus_Western Union Co_20170502-Text.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "213it [00:00, 27607.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Max sentence length: 50\n",
      "Predicting labels for 213 test sentences...\n",
      "Prediction took: 0:00:16\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "scripts = []\n",
    "with open(file_path, 'r') as file:\n",
    "  mydata = file.readlines()\n",
    "  for lines in mydata:\n",
    "    scripts.append(lines)\n",
    "\n",
    "# get sentence segemented review with #sentences > 2\n",
    "def sentence_segment_filter_docs(doc_array):\n",
    "    sentences = []\n",
    "    for doc in nlp.pipe(doc_array, disable=['parser', 'tagger', 'ner'], batch_size=1000, n_threads=8):\n",
    "        sentences.append([sent.text.strip() for sent in doc.sents])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "print(f'Found {len(scripts)} transcripts')\n",
    "print(f'Tokenizing Transcripts...')\n",
    "\n",
    "sentences = sentence_segment_filter_docs(scripts)\n",
    "nr_sents = sum([len(s) for s in sentences])\n",
    "print(f'Segmented {nr_sents} transcript sentences')\n",
    "\n",
    "\n",
    "sentences = sentence_segment_filter_docs(scripts)\n",
    "\n",
    "# Save to file\n",
    "fn_out = f'corpus_{file_name}.txt'\n",
    "\n",
    "with open(fn_out, \"w\") as f:\n",
    "    for sents in tqdm(sentences):\n",
    "        real_sents = []\n",
    "        for s in sents:\n",
    "            x = s.replace(' ', '').replace('\\n', '')\n",
    "            if x != '':\n",
    "                real_sents.append(s.replace('\\n', ''))\n",
    "        # filter only paragraph more than or equal to 1 sentence        \n",
    "        if len(real_sents) >= 1:\n",
    "            str_to_write = \"\\n\".join(real_sents) + \"|||\" + \"\\n\"\n",
    "            f.write(str_to_write)\n",
    "\n",
    "print(f'Done writing to {fn_out}')\n",
    "\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class InputExample(object):\n",
    "\n",
    "    def __init__(self, unique_id, text_a, text_b):\n",
    "        self.unique_id = unique_id\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "\n",
    "corpus = []\n",
    "unique_id = 0\n",
    "count = []\n",
    "with open(fn_out, \"r\", encoding='utf-8') as input_file:\n",
    "  for line in tqdm(input_file):\n",
    "    line = line.strip()\n",
    "    text_a = None\n",
    "    text_b = None\n",
    "    m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "    if m is None:\n",
    "      text_a = re.sub(r\"(\\|\\|\\|)$\", \"\", line)\n",
    "    else:\n",
    "      text_a = m.group(1)\n",
    "      text_b = m.group(2)\n",
    "    corpus.append(InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "    unique_id += 1\n",
    "    cnt = len(line.split())\n",
    "    count.append(cnt)\n",
    "\n",
    "MAX_LEN = int(math.ceil(max(count)/10)*10)\n",
    "print(' ')\n",
    "print('Max sentence length: ' + str(MAX_LEN))\n",
    "\n",
    "# Set the maximum sequence length.\n",
    "# In the original paper, the authors used a length of 512.\n",
    "seq_length = MAX_LEN \n",
    "# type=int\n",
    "# The maximum total input sequence length after WordPiece tokenization. \n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "\n",
    "features = []\n",
    "for (txt_index, sent_pair) in enumerate(corpus):\n",
    "    tokens_a = tokenizer.tokenize(sent_pair.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if sent_pair.text_b:\n",
    "        tokens_b = tokenizer.tokenize(sent_pair.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        input_type_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "        \n",
    "    features.append(InputFeatures(\n",
    "                unique_id=sent_pair.unique_id,\n",
    "                tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                input_type_ids=input_type_ids))\n",
    "    \n",
    "# For Prediction, we try higher batch size of 32\n",
    "\n",
    "batch_size = 32\n",
    "local_rank = -1 \n",
    "#local_rank for distributed training on gpus\n",
    "\n",
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model.\n",
    "unique_id_to_feature = {}\n",
    "for feature in features:\n",
    "    unique_id_to_feature[feature.unique_id] = feature\n",
    "\n",
    "#if local_rank != -1:\n",
    "    #model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "#elif n_gpu > 1:\n",
    "    #model = torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    "# Convert to tensors, need \"input_ids & its index\", \"input_mask\" and \"input_label\"\n",
    "# For testing set\n",
    "prediction_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long) # Token ids for every sentences in individual list\n",
    "prediction_input_ids_index = torch.arange(prediction_input_ids.size(0), dtype=torch.long) # Index for each sentences in one list\n",
    "prediction_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "\n",
    "prediction_data = TensorDataset(prediction_input_ids, prediction_input_mask, prediction_input_ids_index)\n",
    "\n",
    "# Create the DataLoader for our testing set.\n",
    "if local_rank == -1:\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "else:\n",
    "    prediction_sampler = DistributedSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size) \n",
    "# No of item in dataloader = Total sample / Batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions = None\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_input_ids_index = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits, s_logits, e_logits = model(b_input_ids, token_type_ids=None, \n",
    "                                         attention_mask=b_input_mask)\n",
    "\n",
    "      y_prob = logits.softmax(dim = -1) # normalizes values along axis 1\n",
    "      s_y_prob = s_logits.softmax(dim = -1)\n",
    "      e_y_prob = e_logits.softmax(dim = -1)\n",
    "\n",
    "      if predictions is None:\n",
    "        predictions = y_prob.detach().cpu().numpy()\n",
    "        \n",
    "        s_predictions = s_y_prob.detach().cpu().numpy()\n",
    "        s_class = np.argmax(s_predictions, axis=1).flatten()\n",
    "        \n",
    "        e_predictions = e_y_prob.detach().cpu().numpy()\n",
    "        e_class = np.argmax(e_predictions, axis=1).flatten()\n",
    "\n",
    "      else:\n",
    "        predictions = np.concatenate((predictions, y_prob.detach().cpu().numpy()), axis=0)\n",
    "\n",
    "        s_predictions = np.concatenate((s_predictions, s_y_prob.detach().cpu().numpy()), axis=0)\n",
    "        s_class = np.argmax(s_predictions, axis=1).flatten()\n",
    "        \n",
    "        e_predictions = np.concatenate((e_predictions, e_y_prob.detach().cpu().numpy()), axis=0)\n",
    "        e_class = np.argmax(e_predictions, axis=1).flatten()\n",
    "\n",
    "        \n",
    "# Categories of Aspects\n",
    "label_list = [\"sales\",\"earnings\",\"op_costs\",\"products_services\",\"organic_expansion\",\"acquisitions\",\"competition\",\"op_risks\",\"debt\",\"not_applicable\",\"NIL\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "num_labels = len(label_list) # 11\n",
    "\n",
    "# Categories of Sentiment\n",
    "Slabel_list = [\"Negative\",\"Neutral\",\"Positive\",] # Follow order in Slabel_f\n",
    "Slabel2id = {label: i for i, label in enumerate(Slabel_list)}\n",
    "Sid2label = {i: label for i, label in enumerate(Slabel_list)}\n",
    "s_num_labels = len(Slabel_list) # 3\n",
    "\n",
    "# Categories of Emotion\n",
    "Elabel_list = [\"Confident\",\"Dodgy\",\"NIL\",\"Uncertain\"] # Follow order in Elabel_f\n",
    "Elabel2id = {label: i for i, label in enumerate(Elabel_list)}\n",
    "Eid2label = {i: label for i, label in enumerate(Elabel_list)}\n",
    "e_num_labels = len(Elabel_list) # 4\n",
    "\n",
    "\n",
    "# Extract the text used by the tokenizer\n",
    "flat_txt = []\n",
    "for (txt_index, sent_pair) in enumerate(corpus):\n",
    "  txt = sent_pair.text_a\n",
    "  flat_txt.append(txt)\n",
    "\n",
    "# Concat the ids to the Sentiments and Emotion lables\n",
    "s_txt = [str(s)+ \"-\" + Sid2label[int(s)] for s in np.nditer(s_class)]\n",
    "e_txt = [str(e)+ \"-\" + Eid2label[int(e)] for e in np.nditer(e_class)]\n",
    "\n",
    "# Concat the Predictions to a dataframe\n",
    "text_df = pd.DataFrame(data=flat_txt, columns = [\"text\"])\n",
    "s_class_df = pd.DataFrame(data=s_txt, columns = [\"Sentiment\"])\n",
    "e_class_df = pd.DataFrame(data=e_txt, columns = [\"Emotion\"])\n",
    "\n",
    "a_df = pd.DataFrame(data=predictions, columns = list(label2id))\n",
    "\n",
    "s_df = pd.DataFrame(data=s_predictions, columns = list(Slabel2id))\n",
    "\n",
    "e_df = pd.DataFrame(data=e_predictions, columns = list(Elabel2id))\n",
    "\n",
    "output_df = pd.concat([text_df, s_class_df, e_class_df, a_df, s_df, e_df], axis=1)\n",
    "\n",
    "# Saving to CSV\n",
    "pred_name = f'predicted_{file_name}.csv'\n",
    "output_df.to_csv(pred_name, index=True, header=True)\n",
    "    \n",
    "    \n",
    "print(\"Prediction took: {:}\".format(format_time(time.time() - t0)))  \n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IOEYYuMkols"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Aspect Mining Total Probability across 11 Aspects is 0.9990000128746033\n",
      "Sum of Sentiment Classification Total Probability across 3 Sentiment is 1.0\n",
      "Sum of Emotion Classification Total Probability across 4 Emotion is 0.9999999790452421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>sales</th>\n",
       "      <th>earnings</th>\n",
       "      <th>op_costs</th>\n",
       "      <th>products_services</th>\n",
       "      <th>organic_expansion</th>\n",
       "      <th>acquisitions</th>\n",
       "      <th>competition</th>\n",
       "      <th>...</th>\n",
       "      <th>debt</th>\n",
       "      <th>not_applicable</th>\n",
       "      <th>NIL</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Confident</th>\n",
       "      <th>Dodgy</th>\n",
       "      <th>NIL</th>\n",
       "      <th>Uncertain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>We continue to expect full year GAAP earnings ...</td>\n",
       "      <td>2-Positive</td>\n",
       "      <td>0-Confident</td>\n",
       "      <td>0.005799</td>\n",
       "      <td>0.777086</td>\n",
       "      <td>0.101905</td>\n",
       "      <td>0.009286</td>\n",
       "      <td>0.020153</td>\n",
       "      <td>0.015648</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.043345</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.024253</td>\n",
       "      <td>0.970684</td>\n",
       "      <td>0.966125</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.028743</td>\n",
       "      <td>0.003421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>The outstanding share count at quarter end was...</td>\n",
       "      <td>1-Neutral</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.020364</td>\n",
       "      <td>0.693359</td>\n",
       "      <td>0.062885</td>\n",
       "      <td>0.006890</td>\n",
       "      <td>0.027562</td>\n",
       "      <td>0.014818</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.046175</td>\n",
       "      <td>0.112181</td>\n",
       "      <td>0.007759</td>\n",
       "      <td>0.715407</td>\n",
       "      <td>0.276834</td>\n",
       "      <td>0.343142</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.653083</td>\n",
       "      <td>0.001852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>We expect GAAP revenue to be flat to down low-...</td>\n",
       "      <td>0-Negative</td>\n",
       "      <td>2-NIL</td>\n",
       "      <td>0.974048</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.003154</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.991426</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.996732</td>\n",
       "      <td>0.000639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text   Sentiment  \\\n",
       "87  We continue to expect full year GAAP earnings ...  2-Positive   \n",
       "79  The outstanding share count at quarter end was...   1-Neutral   \n",
       "81  We expect GAAP revenue to be flat to down low-...  0-Negative   \n",
       "\n",
       "        Emotion     sales  earnings  op_costs  products_services  \\\n",
       "87  0-Confident  0.005799  0.777086  0.101905           0.009286   \n",
       "79        2-NIL  0.020364  0.693359  0.062885           0.006890   \n",
       "81        2-NIL  0.974048  0.013171  0.003738           0.003154   \n",
       "\n",
       "    organic_expansion  acquisitions  competition  ...      debt  \\\n",
       "87           0.020153      0.015648     0.004748  ...  0.003428   \n",
       "79           0.027562      0.014818     0.004131  ...  0.005030   \n",
       "81           0.002075      0.001391     0.000308  ...  0.000491   \n",
       "\n",
       "    not_applicable       NIL  Negative   Neutral  Positive  Confident  \\\n",
       "87        0.008273  0.043345  0.005062  0.024253  0.970684   0.966125   \n",
       "79        0.046175  0.112181  0.007759  0.715407  0.276834   0.343142   \n",
       "81        0.000652  0.000648  0.991426  0.003097  0.005476   0.001548   \n",
       "\n",
       "       Dodgy       NIL  Uncertain  \n",
       "87  0.001710  0.028743   0.003421  \n",
       "79  0.001924  0.653083   0.001852  \n",
       "81  0.001081  0.996732   0.000639  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.round(predictions, decimals=3)\n",
    "print(\"Sum of Aspect Mining Total Probability across \" + str(len(predictions[0])) + \" Aspects is \" + str(sum(predictions[0])))\n",
    "\n",
    "s_predictions = np.round(s_predictions, decimals=3)\n",
    "print(\"Sum of Sentiment Classification Total Probability across \" + str(len(s_predictions[0])) + \" Sentiment is \" + str(sum(s_predictions[0])))\n",
    "\n",
    "e_predictions = np.round(e_predictions, decimals=3)\n",
    "print(\"Sum of Emotion Classification Total Probability across \" + str(len(e_predictions[0])) + \" Emotion is \" + str(sum(e_predictions[0])))\n",
    "\n",
    "output_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Multitask_Prediction_BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
