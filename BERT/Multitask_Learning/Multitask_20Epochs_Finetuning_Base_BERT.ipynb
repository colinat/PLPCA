{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multitask_20Epochs_Finetuning_Base_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ypk0s-xs69Ha",
        "mkyubuJSOzg3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSU7yERLP_66",
        "colab_type": "text"
      },
      "source": [
        "## Using Colab GPU for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eot6_cHkIxHu",
        "colab_type": "code",
        "outputId": "895cfc1a-187a-428d-b7cc-51e51bca6c1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!rm -rf NLP_Colab\n",
        "!git clone https://github.com/sumkh/NLP_Colab.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP_Colab'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 81 (delta 44), reused 30 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (81/81), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjPyC2CtSUjV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "be478e7e-9b81-4297-bbc7-a35c842245d0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab_type": "code",
        "outputId": "be493642-56fc-4652-9e08-83a9975a79b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab_type": "code",
        "outputId": "03a2d292-ee44-43a5-b692-5e78b41908aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q",
        "colab_type": "code",
        "outputId": "80f03300-38ae-465b-d4f7-84b96142af1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install transformers -q"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 501kB 5.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 16.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 33.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7MB 45.3MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amSbgl4B6u1j",
        "colab_type": "text"
      },
      "source": [
        "# Training and Fine-Tuning BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2qG0uKq6BMs",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Training and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ",
        "colab_type": "code",
        "outputId": "4970f49a-ad49-4cd7-9faf-a8dcceee27d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"NLP_Colab/Multi.csv\")\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 5 random rows from the data.\n",
        "df.sample(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 6,697\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mydata</th>\n",
              "      <th>Aspects</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>sales</th>\n",
              "      <th>earnings</th>\n",
              "      <th>op_costs</th>\n",
              "      <th>products_services</th>\n",
              "      <th>organic_expansion</th>\n",
              "      <th>acquisitions</th>\n",
              "      <th>competition</th>\n",
              "      <th>op_risks</th>\n",
              "      <th>debt</th>\n",
              "      <th>not_applicable</th>\n",
              "      <th>NIL</th>\n",
              "      <th>Slabel_f</th>\n",
              "      <th>Elabel_f</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4005</th>\n",
              "      <td>So, obviously, good quarter this quarter</td>\n",
              "      <td>sales</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Confident</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3404</th>\n",
              "      <td>We don't need to chase growth here, we can be ...</td>\n",
              "      <td>sales</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Confident</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>Note that these incentives are now being faced...</td>\n",
              "      <td>products_services</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Confident</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5084</th>\n",
              "      <td>We're seeing the U.S</td>\n",
              "      <td>NIL</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>NIL</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>Because safety is so critical, the automotive ...</td>\n",
              "      <td>products_services</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NIL</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 mydata  ... Elabel_f\n",
              "4005           So, obviously, good quarter this quarter  ...        0\n",
              "3404  We don't need to chase growth here, we can be ...  ...        0\n",
              "186   Note that these incentives are now being faced...  ...        0\n",
              "5084                               We're seeing the U.S  ...        2\n",
              "199   Because safety is so critical, the automotive ...  ...        2\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8kEDRvShcU5",
        "colab_type": "text"
      },
      "source": [
        "## BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A",
        "colab_type": "code",
        "outputId": "b1bdaeff-3a31-4321-ea61-3aab0dde2463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 08:50:48 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SMZ5T5Imhlx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Let's extract the sentences and labels of our training set as numpy ndarrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhtYMsJumEq6",
        "colab_type": "code",
        "outputId": "2cce0ae6-5237-4786-8f92-3af4a39b8200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Categories of Aspects\n",
        "label_list = [\"sales\",\"earnings\",\"op_costs\",\"products_services\",\"organic_expansion\",\"acquisitions\",\"competition\",\"op_risks\",\"debt\",\"not_applicable\",\"NIL\"]\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "num_labels = len(label_list) # 11\n",
        "\n",
        "# Categories of Sentiment\n",
        "Slabel_list = [\"Negative\",\"Neutral\",\"Positive\",] # Follow order in Slabel_f\n",
        "Slabel2id = {label: i for i, label in enumerate(Slabel_list)}\n",
        "Sid2label = {i: label for i, label in enumerate(Slabel_list)}\n",
        "s_num_labels = len(Slabel_list) # 3\n",
        "\n",
        "# Categories of Emotion\n",
        "Elabel_list = [\"Confident\",\"Dodgy\",\"NIL\",\"Uncertain\"] # Follow order in Elabel_f\n",
        "Elabel2id = {label: i for i, label in enumerate(Elabel_list)}\n",
        "Eid2label = {i: label for i, label in enumerate(Elabel_list)}\n",
        "e_num_labels = len(Elabel_list) # 4\n",
        "\n",
        "id2label"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'sales',\n",
              " 1: 'earnings',\n",
              " 2: 'op_costs',\n",
              " 3: 'products_services',\n",
              " 4: 'organic_expansion',\n",
              " 5: 'acquisitions',\n",
              " 6: 'competition',\n",
              " 7: 'op_risks',\n",
              " 8: 'debt',\n",
              " 9: 'not_applicable',\n",
              " 10: 'NIL'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNh9Fz6vEDAw",
        "colab_type": "code",
        "outputId": "ff19d487-787c-411a-f859-2044a96b4f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Sid2label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Negative', 1: 'Neutral', 2: 'Positive'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBx39NY1EBB4",
        "colab_type": "code",
        "outputId": "64844068-d3ba-4c55-e6ac-67c5ba632ad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Eid2label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Confident', 1: 'Dodgy', 2: 'NIL', 3: 'Uncertain'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUijVxTGodqz",
        "colab_type": "code",
        "outputId": "d8d3969a-7216-483f-ba76-2cfcfdcd9f9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Set labels for Aspect\n",
        "label_f = df[[\"sales\",\"earnings\",\"op_costs\",\"products_services\",\"organic_expansion\",\"acquisitions\",\"competition\",\"op_risks\",\"debt\",\"not_applicable\",\"NIL\"]]\n",
        "label_f = label_f.values.tolist()\n",
        "print(label_list)\n",
        "print(\"Number of Labels: \" + str(num_labels))\n",
        "label_f[0:3]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sales', 'earnings', 'op_costs', 'products_services', 'organic_expansion', 'acquisitions', 'competition', 'op_risks', 'debt', 'not_applicable', 'NIL']\n",
            "Number of Labels: 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuCKRQbqp2im",
        "colab_type": "code",
        "outputId": "4a2d3cf8-5e56-4edc-a9c2-90aa36eba917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Set labels for Sentiment\n",
        "s_label_f = df[[\"Slabel_f\"]]\n",
        "s_label_f = s_label_f.values.tolist()\n",
        "\n",
        "# Set labels for Emotion\n",
        "e_label_f = df[[\"Elabel_f\"]]\n",
        "e_label_f = e_label_f.values.tolist()\n",
        "\n",
        "# Convert text to numpy arrays\n",
        "text = df.iloc[:,0]\n",
        "text = text.values.tolist()\n",
        "text[0:3]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yeah',\n",
              " 'One is more for the data center',\n",
              " 'Premiere Pro is the worlds leading video production solution, and continues to grow its footprint across every video segment']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VglOTCywFZ8",
        "colab_type": "code",
        "outputId": "ba486c61-8cfa-4a1b-b6e7-bdfa5215eeb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import collections\n",
        "import logging\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InputExample(object):\n",
        "\n",
        "    def __init__(self, unique_id, text_a, text_b, labels, s_labels, e_labels):\n",
        "        self.unique_id = unique_id\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.labels = labels\n",
        "        self.s_labels = s_labels\n",
        "        self.e_labels = e_labels\n",
        "\n",
        "# Get the lists of sentences and their labels.\n",
        "input_file = zip(text, label_f, s_label_f, e_label_f)\n",
        "\n",
        "corpus = []\n",
        "unique_id = 0\n",
        "for txt, lab_f, s_lab_f, e_lab_f in input_file:\n",
        "  line = txt.strip()\n",
        "  text_a = None\n",
        "  text_b = None\n",
        "  m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
        "  if m is None:\n",
        "    text_a = line\n",
        "  else:\n",
        "    text_a = m.group(1)\n",
        "    text_b = m.group(2)\n",
        "  \n",
        "  label = [np.float(x) for x in lab_f]\n",
        "  \n",
        "  corpus.append(InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b, \n",
        "                             labels = label, s_labels = s_lab_f, e_labels = e_lab_f))\n",
        "  unique_id += 1\n",
        "\n",
        "MAX_LEN = int(math.ceil(max([len(keys.split()) for keys in text])/10)*10)\n",
        "print('Max sentence length: ' + str(MAX_LEN))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length: 160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOn_qMZj3QTg",
        "colab_type": "code",
        "outputId": "c3b80938-9359-48ba-8134-e0acfef10735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        }
      },
      "source": [
        "# Set the maximum sequence length.\n",
        "# In the original paper, the authors used a length of 512.\n",
        "seq_length = MAX_LEN \n",
        "# type=int\n",
        "# The maximum total input sequence length after WordPiece tokenization. \n",
        "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, unique_id, labels, s_labels, e_labels, tokens, input_ids, input_mask, input_type_ids):\n",
        "        self.unique_id = unique_id\n",
        "        self.labels = labels\n",
        "        self.s_labels = s_labels\n",
        "        self.e_labels = e_labels\n",
        "        self.tokens = tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.input_type_ids = input_type_ids\n",
        "\n",
        "# Reference: corpus.(unique_id, text_a, text_b, labels, s_labels, e_labels)\n",
        "features = []\n",
        "for (txt_index, sent_pair) in enumerate(corpus):\n",
        "    tokens_a = tokenizer.tokenize(sent_pair.text_a)\n",
        "\n",
        "    tokens_b = None\n",
        "    if sent_pair.text_b:\n",
        "        tokens_b = tokenizer.tokenize(sent_pair.text_b)\n",
        "\n",
        "    if tokens_b:\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "        # length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
        "    else:\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > seq_length - 2:\n",
        "            tokens_a = tokens_a[0:(seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    input_type_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    input_type_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        input_type_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    input_type_ids.append(0)\n",
        "\n",
        "    if tokens_b:\n",
        "        for token in tokens_b:\n",
        "            tokens.append(token)\n",
        "            input_type_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        input_type_ids.append(1)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        input_type_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == seq_length\n",
        "    assert len(input_mask) == seq_length\n",
        "    assert len(input_type_ids) == seq_length\n",
        "\n",
        "    if txt_index < 5:\n",
        "        logger.info(\"******\")\n",
        "        logger.info(\"unique_id: %s\" % (sent_pair.unique_id))\n",
        "        logger.info(\"Aspect labels: %s\" % (sent_pair.labels))\n",
        "        logger.info(\"Sentiment labels: %s\" % (sent_pair.s_labels))\n",
        "        logger.info(\"Emotion labels: %s\" % (sent_pair.e_labels))\n",
        "        logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "        logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "        logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "        logger.info(\n",
        "            \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
        "        \n",
        "    features.append(InputFeatures(\n",
        "                unique_id=sent_pair.unique_id,\n",
        "                labels=sent_pair.labels,\n",
        "                s_labels=sent_pair.s_labels,\n",
        "                e_labels=sent_pair.e_labels,\n",
        "                tokens=tokens,\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                input_type_ids=input_type_ids))\n",
        "    "
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 08:51:19 - INFO - __main__ -   ******\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   unique_id: 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Aspect labels: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Sentiment labels: [1]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Emotion labels: [2]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   tokens: [CLS] yeah [SEP]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_ids: 101 3398 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   ******\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   unique_id: 1\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Aspect labels: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Sentiment labels: [1]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Emotion labels: [2]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   tokens: [CLS] one is more for the data center [SEP]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_ids: 101 2028 2003 2062 2005 1996 2951 2415 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   ******\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   unique_id: 2\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Aspect labels: [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Sentiment labels: [2]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Emotion labels: [0]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   tokens: [CLS] premiere pro is the worlds leading video production solution , and continues to grow its footprint across every video segment [SEP]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_ids: 101 6765 4013 2003 1996 8484 2877 2678 2537 5576 1010 1998 4247 2000 4982 2049 24319 2408 2296 2678 6903 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   ******\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   unique_id: 3\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Aspect labels: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Sentiment labels: [1]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Emotion labels: [2]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   tokens: [CLS] if they ' re ds ##a , now they can buy imagine manager [SEP]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_ids: 101 2065 2027 1005 2128 16233 2050 1010 2085 2027 2064 4965 5674 3208 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   ******\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   unique_id: 4\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Aspect labels: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Sentiment labels: [2]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   Emotion labels: [0]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   tokens: [CLS] we also saw orders growth in all regions with particular strength in asia , which grew 26 % year - over - year , with strength in china [SEP]\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_ids: 101 2057 2036 2387 4449 3930 1999 2035 4655 2007 3327 3997 1999 4021 1010 2029 3473 2656 1003 2095 1011 2058 1011 2095 1010 2007 3997 1999 2859 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:51:19 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu0ao7p8rb06",
        "colab_type": "text"
      },
      "source": [
        "Divide up our training set to use 90% for training and 10% for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFbE-UHvsb7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "t_features, v_features = train_test_split(features, random_state=123, test_size=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LzSbTqW9_BR",
        "colab_type": "text"
      },
      "source": [
        "## Converting to PyTorch Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p1uXczp-Je4",
        "colab_type": "text"
      },
      "source": [
        "Our model expects PyTorch tensors rather than numpy.ndarrays, so convert all of our dataset variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD9i6Z2pG-sN",
        "colab_type": "text"
      },
      "source": [
        "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw5K2A5Ko1RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32. \n",
        "# For Colab, recommend batch size of 8\n",
        "\n",
        "batch_size = 8\n",
        "local_rank = -1 \n",
        "#local_rank for distributed training on gpus\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "unique_id_to_feature = {}\n",
        "for feature in t_features:\n",
        "    unique_id_to_feature[feature.unique_id] = feature\n",
        "\n",
        "if local_rank != -1:\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
        "elif n_gpu > 1:\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "\n",
        "# Convert to tensors, need \"input_ids & its index\", \"input_mask\" and \"input_label\"\n",
        "# For Training set\n",
        "t_input_ids = torch.tensor([f.input_ids for f in t_features], dtype=torch.long) # Token ids for every sentences in individual list\n",
        "t_input_ids_index = torch.arange(t_input_ids.size(0), dtype=torch.long) # Index for each sentences in one list\n",
        "t_input_mask = torch.tensor([f.input_mask for f in t_features], dtype=torch.long)\n",
        "\n",
        "# Training and Labels for each task\n",
        "t_input_label = torch.tensor([f.labels for f in t_features], dtype=torch.long)\n",
        "t_input_s_label = torch.tensor([f.s_labels for f in t_features], dtype=torch.long)\n",
        "t_input_e_label = torch.tensor([f.e_labels for f in t_features], dtype=torch.long)\n",
        "\n",
        "train_data = TensorDataset(t_input_ids, t_input_mask, \n",
        "                           t_input_label, t_input_s_label, t_input_e_label, \n",
        "                           t_input_ids_index)\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "if local_rank == -1:\n",
        "    train_sampler = SequentialSampler(train_data)\n",
        "else:\n",
        "    train_sampler = DistributedSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # No of item in dataloader = Total sample / Batch_size\n",
        "\n",
        "\n",
        "# For Validation set\n",
        "v_input_ids = torch.tensor([f.input_ids for f in v_features], dtype=torch.long) # Token ids for every sentences in individual list\n",
        "v_input_ids_index = torch.arange(v_input_ids.size(0), dtype=torch.long) # Index for each sentences in one list\n",
        "v_input_mask = torch.tensor([f.input_mask for f in v_features], dtype=torch.long)\n",
        "\n",
        "# Validation and Labels for each task\n",
        "v_input_label = torch.tensor([f.labels for f in v_features], dtype=torch.long)\n",
        "v_input_s_label = torch.tensor([f.s_labels for f in v_features], dtype=torch.long)\n",
        "v_input_e_label = torch.tensor([f.e_labels for f in v_features], dtype=torch.long)\n",
        "\n",
        "validation_data = TensorDataset(v_input_ids, v_input_mask, \n",
        "                                v_input_label, v_input_s_label, v_input_e_label,\n",
        "                                v_input_ids_index)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "if local_rank == -1:\n",
        "    validation_sampler = SequentialSampler(validation_data)\n",
        "else:\n",
        "    validation_sampler = DistributedSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size) # No of item in dataloader = Total sample / Batch_size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdldbRhq1gGj",
        "colab_type": "text"
      },
      "source": [
        "## Defining the Classification Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc9G4IqGzWQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from transformers.modeling_bert import BertPreTrainedModel, BertModel\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "class BertForMultitask(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertForMultitask, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        self.classifier = nn.Linear(config.hidden_size, out_features=11)\n",
        "        self.s_classifier = nn.Linear(config.hidden_size, out_features=3)\n",
        "        self.e_classifier = nn.Linear(config.hidden_size, out_features=4)\n",
        "\n",
        "        # define dropout layer in __init__\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None, \n",
        "                labels=None, s_labels=None, e_labels=None):\n",
        "        _, pooled_output  = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        s_logits = self.s_classifier(pooled_output)\n",
        "\n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        e_logits = self.e_classifier(pooled_output)\n",
        "        \n",
        "        # When training, provides label and loss will be included into output, \n",
        "        # for validation, no label will be entered\n",
        "        if labels is not None:\n",
        "          loss_fct = BCEWithLogitsLoss() ## pos_weight=[1,1,1,1,1,1,1,1,1,1,1]\n",
        "          loss = loss_fct(logits.float(), labels.float())\n",
        "        \n",
        "          s_loss_fct = CrossEntropyLoss()\n",
        "          s_loss = s_loss_fct(s_logits.view(-1, 3), s_labels.view(-1))\n",
        "          \n",
        "          e_loss_fct = CrossEntropyLoss()\n",
        "          e_loss = e_loss_fct(e_logits.view(-1, 4), e_labels.view(-1))\n",
        "        \n",
        "          # view function is meant to reshape the tensor. \n",
        "          # If there is any situation that you don't know how many rows you \n",
        "          # want but are sure of the number of columns (i.e. num_label), \n",
        "          # then you can specify this with a -1 so that the logits are reshaped\n",
        "          # to matrix of 1xNum_label\n",
        "        \n",
        "          losses = loss + s_loss + e_loss\n",
        "          return losses\n",
        "        \n",
        "        else: \n",
        "          all_logits = logits, s_logits, e_logits\n",
        "          return all_logits\n",
        "\n",
        "        outputs = (losses,) + all_logits\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def freeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def unfreeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDqUq7Eizva1",
        "colab_type": "text"
      },
      "source": [
        "### Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR4OqsNEr4Vw",
        "colab_type": "code",
        "outputId": "4bdd4a98-241a-4fa3-c554-ba28cd72263e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import AdamW, BertConfig\n",
        "\n",
        "# Load Custom layer for BertForMultitask, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForMultitask.from_pretrained(\"bert-base-uncased\")   \n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 08:51:56 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/24/2020 08:51:56 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/24/2020 08:51:57 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/24/2020 08:51:59 - INFO - transformers.modeling_utils -   Weights of BertForMultitask not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 's_classifier.weight', 's_classifier.bias', 'e_classifier.weight', 'e_classifier.bias']\n",
            "03/24/2020 08:51:59 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMultitask: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMultitask(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
              "  (s_classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              "  (e_classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u-0yImuPCZF",
        "colab_type": "text"
      },
      "source": [
        "### Model's Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Jv6c7-HHDW",
        "colab_type": "text"
      },
      "source": [
        "Just for curiosity's sake, we can browse all of the model's parameters by name here.\n",
        "\n",
        "In the below cell, I've printed out the names and dimensions of the weights for:\n",
        "\n",
        "1. The embedding layer.\n",
        "2. The first of the twelve transformers.\n",
        "3. The output layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIiVlDYCtSq",
        "colab_type": "code",
        "outputId": "5cb336ba-054b-412a-ec71-76ce1231c251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-8:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 205 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                          (11, 768)\n",
            "classifier.bias                                                (11,)\n",
            "s_classifier.weight                                         (3, 768)\n",
            "s_classifier.bias                                               (3,)\n",
            "e_classifier.weight                                         (4, 768)\n",
            "e_classifier.bias                                               (4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWT-D4U_Pvx",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o-VEBobKwHk",
        "colab_type": "text"
      },
      "source": [
        "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
        "\n",
        "For the purposes of fine-tuning, the authors recommend choosing from the following values:\n",
        "- Batch size: 16, 32  (We chose **8** when creating our DataLoaders).\n",
        "- Learning rate (Adam): 5e-5, 3e-5, 2e-5  (**We'll use 2e-5**).\n",
        "- Number of epochs: 2, 3, 4  (**We'll use 4**).\n",
        "\n",
        "The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
        "\n",
        "You can find the creation of the AdamW optimizer in `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 20\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfmWwUR_Sox",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE5B99H5H2-W",
        "colab_type": "text"
      },
      "source": [
        "Define a helper function for calculating accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw3OaPY0sYD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Reference Compute Softmax function using numpy:\n",
        "# https://gist.github.com/alceufc/f3fd0cd7d9efb120195c\n",
        "def np_softmax(x):\n",
        "    scoreMatExp = np.exp(np.asarray(x))\n",
        "    return scoreMatExp / scoreMatExp.sum(0)\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    preds = np_softmax(preds)\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def accuracy_thresh(y_pred, y_true, thresh=0.5, sigmoid=True):\n",
        "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
        "    if sigmoid: y_pred = y_pred.sigmoid()\n",
        "    return np.mean(((y_pred>thresh)==y_true.byte()).float().cpu().numpy(), axis=1).sum()\n",
        "\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab_type": "code",
        "outputId": "df159e63-9cd5-4769-97e9-af56d714e572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 123\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "eval_loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels (Label for Aspect Mining task)\n",
        "        #   [3]: s_label (Label for Sentiment Classification task)\n",
        "        #   [4]: e_label (Label for Emotion Classification task)\n",
        "        #   [5]: input_ids_index\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_s_labels = batch[3].to(device)\n",
        "        b_e_labels = batch[4].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask,\n",
        "                        labels=b_labels, \n",
        "                        s_labels=b_s_labels, \n",
        "                        e_labels=b_e_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs\n",
        "        \n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.mean().item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    all_logits = None\n",
        "    all_labels = None\n",
        "     \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    s_eval_loss, s_eval_accuracy = 0, 0\n",
        "    e_eval_loss, e_eval_accuracy = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels, b_s_labels, b_e_labels, b_input_ids_index = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            \n",
        "            #logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            logits, s_logits, e_logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax (here BCEWithLogitsLoss).\n",
        "            loss_fct = BCEWithLogitsLoss() # pos_weight=[1,1,1,1,1,1,1,1,1,1,1]\n",
        "            tmp_eval_loss = loss_fct(logits.float(), b_labels.float())\n",
        "            \n",
        "            # For Sentiment and Emotion Validation:  \n",
        "            s_loss_fct = CrossEntropyLoss()\n",
        "            tmp_s_eval_loss = s_loss_fct(s_logits.view(-1, 3), b_s_labels.view(-1))\n",
        "          \n",
        "            e_loss_fct = CrossEntropyLoss()\n",
        "            tmp_e_eval_loss = e_loss_fct(e_logits.view(-1, 4), b_e_labels.view(-1))\n",
        "        \n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            s_logits = s_logits.detach().cpu().numpy()\n",
        "            s_label_ids = b_s_labels.to('cpu').numpy()\n",
        "\n",
        "            e_logits = e_logits.detach().cpu().numpy()\n",
        "            e_label_ids = b_e_labels.to('cpu').numpy()\n",
        "        \n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            s_tmp_eval_accuracy = flat_accuracy(s_logits, s_label_ids)\n",
        "            e_tmp_eval_accuracy = flat_accuracy(e_logits, e_label_ids)\n",
        "        \n",
        "            # Accumulate the total accuracy.\n",
        "            s_eval_accuracy += s_tmp_eval_accuracy\n",
        "            e_eval_accuracy += e_tmp_eval_accuracy\n",
        "\n",
        "          \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = accuracy_thresh(logits, b_labels)\n",
        "        if all_logits is None:\n",
        "          all_logits = logits.detach().cpu().numpy()\n",
        "        else:\n",
        "          all_logits = np.concatenate((all_logits, logits.detach().cpu().numpy()), axis=0)\n",
        "            \n",
        "        if all_labels is None:\n",
        "          all_labels = b_labels.detach().cpu().numpy()\n",
        "        else:    \n",
        "          all_labels = np.concatenate((all_labels, b_labels.detach().cpu().numpy()), axis=0)\n",
        "        \n",
        "        # Average evaluation loss per batch\n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        s_eval_loss += tmp_s_eval_loss.mean().item()\n",
        "        e_eval_loss += tmp_e_eval_loss.mean().item()\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        nb_eval_examples += b_labels.size(0)\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps # total loss/no. of batches\n",
        "    eval_accuracy = eval_accuracy / nb_eval_examples # total accuracy of each aspect / no. of aspect (11 x no.of batches)\n",
        "\n",
        "    s_eval_loss = s_eval_loss / nb_eval_steps \n",
        "    e_eval_loss = e_eval_loss / nb_eval_steps\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    total_eval_loss = (eval_loss + s_eval_loss + e_eval_loss)/3\n",
        "\n",
        "    # Store the loss value for plotting the validation curve.\n",
        "    eval_loss_values.append(total_eval_loss)\n",
        "\n",
        "        \n",
        "    # ROC-AUC calcualation\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    \n",
        "    for i in range(num_labels):\n",
        "        fpr[i], tpr[i], _ = roc_curve(all_labels[:, i], all_logits[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        \n",
        "    # Compute micro-average ROC curve and ROC area\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels.ravel(), all_logits.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    result = {'eval_loss': eval_loss,\n",
        "              'eval_accuracy': eval_accuracy,\n",
        "              'roc_auc': roc_auc }\n",
        "    \n",
        "    print(\"Aspects Mining:\")\n",
        "    print(\"  Loss:   {0:.2f}\".format(eval_loss) + \"    Accuracy:   {0:.2f}\".format(eval_accuracy))\n",
        "    #print(\"  Validation Accuracy: {0:.2f}\".format(eval_accuracy))\n",
        "    print(\"  ROC AUC: \")\n",
        "    print([str(i) + \":\" + str(int(roc_auc[i]*1000)/10)+\"%\" for i in roc_auc])\n",
        "    print(\"Sentiment Classification:\")\n",
        "    print(\"  Loss:   {0:.2f}\".format(s_eval_loss) + \"    Accuracy:   {0:.2f}\".format(s_eval_accuracy/nb_eval_steps))\n",
        "    #print(\"  Validation Accuracy: {0:.2f}\".format(s_eval_accuracy/nb_eval_steps))\n",
        "    print(\"Emotion Classification:\")\n",
        "    print(\"  Loss:   {0:.2f}\".format(e_eval_loss) + \"    Accuracy:   {0:.2f}\".format(e_eval_accuracy/nb_eval_steps))\n",
        "    #print(\"  Validation Accuracy: {0:.2f}\".format(e_eval_accuracy/nb_eval_steps))\n",
        "    print(\"\")\n",
        "    print(\"  Average Validation Loss: {0:.2f}\".format(total_eval_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:20.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:33.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:46.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:59.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:12.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:25.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:31.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:38.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:44.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:51.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 1.50\n",
            "  Training epcoh took: 0:02:02\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.22    Accuracy:   0.93\n",
            "  ROC AUC: \n",
            "['0:90.4%', '1:73.4%', '2:50.6%', '3:69.6%', '4:68.5%', '5:55.0%', '6:56.2%', '7:37.6%', '8:46.7%', '9:96.3%', '10:56.0%', 'micro:87.2%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.45    Accuracy:   0.75\n",
            "Emotion Classification:\n",
            "  Loss:   0.57    Accuracy:   0.70\n",
            "\n",
            "  Average Validation Loss: 0.41\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 2 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:24.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:31.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:37.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:44.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:50.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epcoh took: 0:02:02\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.20    Accuracy:   0.93\n",
            "  ROC AUC: \n",
            "['0:91.0%', '1:75.8%', '2:66.6%', '3:71.9%', '4:71.0%', '5:67.9%', '6:66.0%', '7:42.4%', '8:61.5%', '9:96.9%', '10:67.7%', 'micro:89.5%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.45    Accuracy:   0.75\n",
            "Emotion Classification:\n",
            "  Loss:   0.53    Accuracy:   0.69\n",
            "\n",
            "  Average Validation Loss: 0.39\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 3 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:24.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:31.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:37.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:44.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:50.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.86\n",
            "  Training epcoh took: 0:02:02\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.20    Accuracy:   0.93\n",
            "  ROC AUC: \n",
            "['0:91.7%', '1:78.7%', '2:75.6%', '3:75.4%', '4:77.9%', '5:76.0%', '6:66.9%', '7:75.4%', '8:78.5%', '9:97.0%', '10:73.2%', 'micro:90.8%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.50    Accuracy:   0.76\n",
            "Emotion Classification:\n",
            "  Loss:   0.59    Accuracy:   0.71\n",
            "\n",
            "  Average Validation Loss: 0.43\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 4 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:24.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:31.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:37.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:44.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:50.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:02:02\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.19    Accuracy:   0.93\n",
            "  ROC AUC: \n",
            "['0:93.3%', '1:80.3%', '2:82.1%', '3:76.1%', '4:78.0%', '5:74.9%', '6:67.2%', '7:83.3%', '8:84.9%', '9:97.2%', '10:75.9%', 'micro:91.6%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.55    Accuracy:   0.76\n",
            "Emotion Classification:\n",
            "  Loss:   0.65    Accuracy:   0.70\n",
            "\n",
            "  Average Validation Loss: 0.46\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 5 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:24.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:31.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:37.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:44.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:50.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epcoh took: 0:02:02\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.18    Accuracy:   0.93\n",
            "  ROC AUC: \n",
            "['0:94.1%', '1:84.2%', '2:81.9%', '3:79.7%', '4:80.1%', '5:79.4%', '6:65.9%', '7:81.6%', '8:87.9%', '9:97.5%', '10:76.6%', 'micro:92.2%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.69    Accuracy:   0.75\n",
            "Emotion Classification:\n",
            "  Loss:   0.75    Accuracy:   0.68\n",
            "\n",
            "  Average Validation Loss: 0.54\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 6 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:20.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:33.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:46.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:59.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:12.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:25.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:31.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:38.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:44.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:51.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epcoh took: 0:02:03\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.18    Accuracy:   0.94\n",
            "  ROC AUC: \n",
            "['0:94.9%', '1:86.8%', '2:86.2%', '3:87.5%', '4:77.7%', '5:84.8%', '6:67.5%', '7:83.2%', '8:88.5%', '9:97.1%', '10:76.0%', 'micro:92.8%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.79    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   0.94    Accuracy:   0.66\n",
            "\n",
            "  Average Validation Loss: 0.64\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 7 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:20.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:33.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:46.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:59.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:12.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:25.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:31.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:38.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:44.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:51.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:02:03\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.16    Accuracy:   0.95\n",
            "  ROC AUC: \n",
            "['0:96.1%', '1:89.5%', '2:87.8%', '3:91.8%', '4:81.8%', '5:85.5%', '6:69.8%', '7:76.5%', '8:90.5%', '9:97.0%', '10:80.3%', 'micro:93.8%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.82    Accuracy:   0.75\n",
            "Emotion Classification:\n",
            "  Loss:   0.92    Accuracy:   0.65\n",
            "\n",
            "  Average Validation Loss: 0.63\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 8 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:24.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:31.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:37.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:44.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:50.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epcoh took: 0:02:02\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.16    Accuracy:   0.95\n",
            "  ROC AUC: \n",
            "['0:96.0%', '1:90.4%', '2:88.9%', '3:93.5%', '4:84.4%', '5:86.2%', '6:72.8%', '7:79.0%', '8:86.7%', '9:97.2%', '10:81.0%', 'micro:94.2%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.86    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   1.00    Accuracy:   0.64\n",
            "\n",
            "  Average Validation Loss: 0.67\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 9 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:24.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:31.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:37.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:44.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:50.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epcoh took: 0:02:02\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.15    Accuracy:   0.95\n",
            "  ROC AUC: \n",
            "['0:96.1%', '1:91.5%', '2:90.8%', '3:94.8%', '4:85.9%', '5:91.0%', '6:71.8%', '7:81.6%', '8:85.8%', '9:97.2%', '10:82.7%', 'micro:94.7%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.85    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   1.01    Accuracy:   0.63\n",
            "\n",
            "  Average Validation Loss: 0.67\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 10 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:17.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:24.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:30.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:37.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:43.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:50.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:56.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:02:02\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.15    Accuracy:   0.95\n",
            "  ROC AUC: \n",
            "['0:96.2%', '1:92.5%', '2:92.5%', '3:94.8%', '4:89.6%', '5:95.0%', '6:72.2%', '7:84.1%', '8:87.8%', '9:97.4%', '10:82.7%', 'micro:95.1%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.85    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   1.05    Accuracy:   0.63\n",
            "\n",
            "  Average Validation Loss: 0.68\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 11 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:17.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:24.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:30.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:37.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:43.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:50.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:56.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:02:02\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.14    Accuracy:   0.96\n",
            "  ROC AUC: \n",
            "['0:95.9%', '1:92.5%', '2:93.8%', '3:96.0%', '4:89.7%', '5:97.3%', '6:75.5%', '7:83.7%', '8:85.1%', '9:97.2%', '10:84.0%', 'micro:95.4%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.87    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   0.96    Accuracy:   0.61\n",
            "\n",
            "  Average Validation Loss: 0.66\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 12 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:05.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:17.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:24.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:30.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:37.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:43.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:50.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:56.\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 0:02:02\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.14    Accuracy:   0.96\n",
            "  ROC AUC: \n",
            "['0:96.2%', '1:92.6%', '2:94.4%', '3:95.8%', '4:90.9%', '5:99.1%', '6:79.4%', '7:82.6%', '8:88.2%', '9:96.9%', '10:84.3%', 'micro:95.4%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.86    Accuracy:   0.75\n",
            "Emotion Classification:\n",
            "  Loss:   1.07    Accuracy:   0.63\n",
            "\n",
            "  Average Validation Loss: 0.69\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 13 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:52.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:17.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:24.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:30.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:37.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:43.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:49.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:56.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:02:01\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.14    Accuracy:   0.96\n",
            "  ROC AUC: \n",
            "['0:96.1%', '1:93.0%', '2:95.0%', '3:95.7%', '4:92.0%', '5:99.3%', '6:80.7%', '7:86.9%', '8:89.0%', '9:96.5%', '10:85.2%', 'micro:95.5%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.92    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   1.06    Accuracy:   0.62\n",
            "\n",
            "  Average Validation Loss: 0.71\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 14 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:51.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:17.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:24.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:30.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:36.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:43.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:49.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:56.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:02:01\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.14    Accuracy:   0.96\n",
            "  ROC AUC: \n",
            "['0:96.0%', '1:93.1%', '2:94.3%', '3:94.9%', '4:92.0%', '5:99.7%', '6:82.3%', '7:83.6%', '8:89.6%', '9:96.8%', '10:85.3%', 'micro:95.5%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.89    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   1.07    Accuracy:   0.62\n",
            "\n",
            "  Average Validation Loss: 0.70\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 15 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:39.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:51.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:17.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:23.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:30.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:36.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:43.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:49.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:56.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:02:01\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.14    Accuracy:   0.96\n",
            "  ROC AUC: \n",
            "['0:96.0%', '1:93.0%', '2:93.3%', '3:95.1%', '4:92.0%', '5:99.8%', '6:82.7%', '7:79.6%', '8:92.1%', '9:96.9%', '10:85.4%', 'micro:95.5%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.89    Accuracy:   0.75\n",
            "Emotion Classification:\n",
            "  Loss:   1.08    Accuracy:   0.62\n",
            "\n",
            "  Average Validation Loss: 0.70\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 16 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:38.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:51.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:17.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:23.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:30.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:36.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:43.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:49.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:55.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:02:01\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.15    Accuracy:   0.96\n",
            "  ROC AUC: \n",
            "['0:96.1%', '1:93.2%', '2:94.0%', '3:95.1%', '4:92.1%', '5:99.9%', '6:83.4%', '7:81.8%', '8:91.3%', '9:96.5%', '10:85.5%', 'micro:95.5%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.90    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   1.13    Accuracy:   0.62\n",
            "\n",
            "  Average Validation Loss: 0.73\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 17 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:38.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:51.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:17.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:23.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:30.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:36.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:43.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:49.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:55.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:02:01\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.15    Accuracy:   0.96\n",
            "  ROC AUC: \n",
            "['0:96.2%', '1:93.1%', '2:93.7%', '3:95.1%', '4:92.6%', '5:99.9%', '6:83.0%', '7:80.0%', '8:90.6%', '9:96.6%', '10:85.3%', 'micro:95.5%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.88    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   1.13    Accuracy:   0.61\n",
            "\n",
            "  Average Validation Loss: 0.72\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 18 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:38.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:51.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:10.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:17.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:23.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:30.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:36.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:42.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:49.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:55.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:02:01\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.15    Accuracy:   0.96\n",
            "  ROC AUC: \n",
            "['0:96.2%', '1:93.2%', '2:93.5%', '3:95.2%', '4:91.7%', '5:99.9%', '6:81.9%', '7:78.9%', '8:91.3%', '9:96.7%', '10:85.6%', 'micro:95.6%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.87    Accuracy:   0.75\n",
            "Emotion Classification:\n",
            "  Loss:   1.11    Accuracy:   0.61\n",
            "\n",
            "  Average Validation Loss: 0.71\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 19 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:38.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:51.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:10.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:17.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:23.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:30.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:36.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:42.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:49.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:55.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:02:00\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.15    Accuracy:   0.96\n",
            "  ROC AUC: \n",
            "['0:96.3%', '1:93.1%', '2:93.4%', '3:95.2%', '4:91.9%', '5:99.9%', '6:83.3%', '7:80.5%', '8:91.1%', '9:96.7%', '10:85.6%', 'micro:95.6%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.89    Accuracy:   0.75\n",
            "Emotion Classification:\n",
            "  Loss:   1.11    Accuracy:   0.61\n",
            "\n",
            "  Average Validation Loss: 0.72\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 20 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:19.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:38.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:45.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:51.\n",
            "  Batch   360  of    754.    Elapsed: 0:00:58.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:10.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:17.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:23.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:29.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:36.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:42.\n",
            "  Batch   680  of    754.    Elapsed: 0:01:49.\n",
            "  Batch   720  of    754.    Elapsed: 0:01:55.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:02:00\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.15    Accuracy:   0.96\n",
            "  ROC AUC: \n",
            "['0:96.3%', '1:93.1%', '2:93.4%', '3:95.0%', '4:91.6%', '5:99.9%', '6:83.0%', '7:79.4%', '8:91.8%', '9:96.8%', '10:85.4%', 'micro:95.5%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.89    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   1.13    Accuracy:   0.61\n",
            "\n",
            "  Average Validation Loss: 0.72\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab_type": "code",
        "outputId": "04c60c19-33b2-4575-b597-f911e5fbbd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'r-o')\n",
        "plt.plot(eval_loss_values, 'g-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss [Red] vs Validation loss [Green]\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAGaCAYAAACopj13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkREREQksv8Po3R19MjLNJAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2079Qyn8Mt8",
        "colab_type": "text"
      },
      "source": [
        "# Saving Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYErKiOw4q__",
        "colab_type": "text"
      },
      "source": [
        "This first cell (taken from `run_glue.py` [here](https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495)) writes the model and tokenizer out to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulTWaOr8QNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "9a12da26-1138-4b67-8bee-fecf8abed493"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 09:51:15 - INFO - transformers.configuration_utils -   Configuration saved in ./model_save/config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 09:51:17 - INFO - transformers.modeling_utils -   Model weights saved in ./model_save/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-tjHkR7lc1I",
        "colab_type": "text"
      },
      "source": [
        "Let's check out the file sizes, out of curiosity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqMzI3VTCZo5",
        "colab_type": "code",
        "outputId": "505555e4-d0a5-4139-af98-fa553b7bff9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 428012K\n",
            "-rw-r--r-- 1 root root      1K Mar 24 09:51 config.json\n",
            "-rw-r--r-- 1 root root 427767K Mar 24 09:51 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root      1K Mar 24 09:51 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1K Mar 24 09:51 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    227K Mar 24 09:51 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr_bt2rFlgDn",
        "colab_type": "text"
      },
      "source": [
        "The largest file is the model weights, at around 418 megabytes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUFUIQ8Cu8D",
        "colab_type": "code",
        "outputId": "95bab2d5-e34c-423f-b6da-896e55af0de5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 418M Mar 24 09:51 ./model_save/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzGKvOFAll_e",
        "colab_type": "text"
      },
      "source": [
        "To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxlZsafTC-V5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f2fd0784-d6c7-44e1-ee80-f8ee539962b4"
      },
      "source": [
        "# Zip up the folder and copy zip file to Google Drive\n",
        "!zip -r /content/modelBase_Multitask.zip /content/model_save/\n",
        "!cp -r /content/modelBase_Multitask.zip \"./drive/My Drive/Downloads\""
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/model_save/ (stored 0%)\n",
            "  adding: content/model_save/tokenizer_config.json (stored 0%)\n",
            "  adding: content/model_save/config.json (deflated 56%)\n",
            "  adding: content/model_save/special_tokens_map.json (deflated 40%)\n",
            "  adding: content/model_save/vocab.txt (deflated 53%)\n",
            "  adding: content/model_save/pytorch_model.bin (deflated 7%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypk0s-xs69Ha",
        "colab_type": "text"
      },
      "source": [
        "# Loading Saved Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0vstijw85SZ",
        "colab_type": "text"
      },
      "source": [
        "The following functions will load the model back from disk.  \n",
        "First load the classification layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "14CQo7-RAKRC",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from transformers.modeling_bert import BertPreTrainedModel, BertModel\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "class BertForMultitask(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertForMultitask, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        self.classifier = nn.Linear(config.hidden_size, out_features=11)\n",
        "        self.s_classifier = nn.Linear(config.hidden_size, out_features=3)\n",
        "        self.e_classifier = nn.Linear(config.hidden_size, out_features=4)\n",
        "\n",
        "        # define dropout layer in __init__\n",
        "        #self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None):\n",
        "        _, pooled_output  = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        #pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        #pooled_output = self.dropout(pooled_output)\n",
        "        s_logits = self.s_classifier(pooled_output)\n",
        "\n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        #pooled_output = self.dropout(pooled_output)\n",
        "        e_logits = self.e_classifier(pooled_output)\n",
        "        \n",
        "        outputs = logits, s_logits, e_logits\n",
        "        return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nskPzUM084zL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2375a1d3-fc80-444e-8fa6-42a8599b97a1"
      },
      "source": [
        "#output_dir = './drive/My Drive/EBAC_G/NLP_Project/Embeddings/model_Multitask/'\n",
        "output_dir = '/content/model_save/'\n",
        "\n",
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = BertForMultitask.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 08:46:00 - INFO - transformers.configuration_utils -   loading configuration file /content/model_save/config.json\n",
            "03/24/2020 08:46:00 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMultitask\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/24/2020 08:46:00 - INFO - transformers.modeling_utils -   loading weights file /content/model_save/pytorch_model.bin\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   Model name '/content/model_save/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/content/model_save/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   Didn't find file /content/model_save/added_tokens.json. We won't load it.\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   loading file /content/model_save/vocab.txt\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   loading file /content/model_save/special_tokens_map.json\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   loading file /content/model_save/tokenizer_config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMultitask(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
              "  (s_classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              "  (e_classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3",
        "colab_type": "text"
      },
      "source": [
        "# Performance of Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWe0_JW21MyV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xoy5f3baPXx",
        "colab_type": "code",
        "outputId": "d872cb47-3a5c-47c0-a17e-8b373cd8c55d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import spacy\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoROEEJBN5bE",
        "colab_type": "code",
        "outputId": "4ada6871-1761-4972-ab9b-ce3a356d2666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "file_path = \"drive/My Drive/EBAC_G/NLP_Project/Embeddings/Western Union Co_20170502-Text.txt\"\n",
        "file_name = os.path.basename(file_path)\n",
        "file_name = os.path.splitext(file_name)[0]\n",
        "\n",
        "scripts = []\n",
        "with open(file_path, 'r') as file:\n",
        "  mydata = file.readlines()\n",
        "  for lines in mydata:\n",
        "    scripts.append(lines)\n",
        "\n",
        "# get sentence segemented review with #sentences > 2\n",
        "def sentence_segment_filter_docs(doc_array):\n",
        "    sentences = []\n",
        "    for doc in nlp.pipe(doc_array, disable=['parser', 'tagger', 'ner'], batch_size=1000, n_threads=8):\n",
        "        sentences.append([sent.text.strip() for sent in doc.sents])\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "print(f'Found {len(scripts)} transcripts')\n",
        "print(f'Tokenizing Transcripts...')\n",
        "\n",
        "sentences = sentence_segment_filter_docs(scripts)\n",
        "nr_sents = sum([len(s) for s in sentences])\n",
        "print(f'Segmented {nr_sents} transcript sentences')\n",
        "\n",
        "\n",
        "sentences = sentence_segment_filter_docs(scripts)\n",
        "\n",
        "# Save to file\n",
        "fn_out = f'corpus_{file_name}.txt'\n",
        "\n",
        "with open(fn_out, \"w\") as f:\n",
        "    for sents in tqdm(sentences):\n",
        "        real_sents = []\n",
        "        for s in sents:\n",
        "            x = s.replace(' ', '').replace('\\n', '')\n",
        "            if x != '':\n",
        "                real_sents.append(s.replace('\\n', ''))\n",
        "        # filter only paragraph more than or equal to 1 sentence        \n",
        "        if len(real_sents) >= 1:\n",
        "            str_to_write = \"\\n\".join(real_sents) + \"|||\" + \"\\n\"\n",
        "            f.write(str_to_write)\n",
        "\n",
        "print(f'Done writing to {fn_out}')\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 205 transcripts\n",
            "Tokenizing Transcripts...\n",
            "Segmented 213 transcript sentences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 205/205 [00:00<00:00, 68775.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done writing to corpus_Western Union Co_20170502-Text.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7cpjyGni2_O",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing the Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0q7Y3vog7wM",
        "colab_type": "code",
        "outputId": "67a378c2-738f-4d8e-f16c-483dbe44f272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "import argparse\n",
        "import collections\n",
        "import logging\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InputExample(object):\n",
        "\n",
        "    def __init__(self, unique_id, text_a, text_b):\n",
        "        self.unique_id = unique_id\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "\n",
        "corpus = []\n",
        "unique_id = 0\n",
        "count = []\n",
        "with open(fn_out, \"r\", encoding='utf-8') as input_file:\n",
        "  for line in tqdm(input_file):\n",
        "    line = line.strip()\n",
        "    text_a = None\n",
        "    text_b = None\n",
        "    m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
        "    if m is None:\n",
        "      text_a = re.sub(r\"(\\|\\|\\|)$\", \"\", line)\n",
        "    else:\n",
        "      text_a = m.group(1)\n",
        "      text_b = m.group(2)\n",
        "    corpus.append(InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
        "    unique_id += 1\n",
        "    cnt = len(line.split())\n",
        "    count.append(cnt)\n",
        "\n",
        "MAX_LEN = int(math.ceil(max(count)/10)*10)\n",
        "print(' ')\n",
        "print('Max sentence length: ' + str(MAX_LEN))\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# In the original paper, the authors used a length of 512.\n",
        "seq_length = MAX_LEN \n",
        "# type=int\n",
        "# The maximum total input sequence length after WordPiece tokenization. \n",
        "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
        "        self.unique_id = unique_id\n",
        "        self.tokens = tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.input_type_ids = input_type_ids\n",
        "\n",
        "features = []\n",
        "for (txt_index, sent_pair) in enumerate(corpus):\n",
        "    tokens_a = tokenizer.tokenize(sent_pair.text_a)\n",
        "\n",
        "    tokens_b = None\n",
        "    if sent_pair.text_b:\n",
        "        tokens_b = tokenizer.tokenize(sent_pair.text_b)\n",
        "\n",
        "    if tokens_b:\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "        # length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
        "    else:\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > seq_length - 2:\n",
        "            tokens_a = tokens_a[0:(seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    input_type_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    input_type_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        input_type_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    input_type_ids.append(0)\n",
        "\n",
        "    if tokens_b:\n",
        "        for token in tokens_b:\n",
        "            tokens.append(token)\n",
        "            input_type_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        input_type_ids.append(1)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        input_type_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == seq_length\n",
        "    assert len(input_mask) == seq_length\n",
        "    assert len(input_type_ids) == seq_length\n",
        "\n",
        "    if txt_index < 5:\n",
        "        logger.info(\"******\")\n",
        "        logger.info(\"unique_id: %s\" % (sent_pair.unique_id))\n",
        "        logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "        logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "        logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "        logger.info(\"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
        "    \n",
        "        \n",
        "    features.append(InputFeatures(\n",
        "                unique_id=sent_pair.unique_id,\n",
        "                tokens=tokens,\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                input_type_ids=input_type_ids))\n",
        "    "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "213it [00:00, 56954.40it/s]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   ******\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   unique_id: 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   tokens: [CLS] thank you , hi ##km ##et [SEP]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_ids: 101 4067 2017 1010 7632 22287 3388 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   ******\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   unique_id: 1\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   tokens: [CLS] first quarter reported revenues of $ 1 . 3 billion were flat or increased 3 % on a constant currency basis compared to the prior - year period [SEP]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_ids: 101 2034 4284 2988 12594 1997 1002 1015 1012 1017 4551 2020 4257 2030 3445 1017 1003 2006 1037 5377 9598 3978 4102 2000 1996 3188 1011 2095 2558 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   ******\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   unique_id: 2\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   tokens: [CLS] the impact of currency translation , net of hedge benefits , reduced first quarter revenue by approximately $ 30 million compared to the prior year [SEP]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_ids: 101 1996 4254 1997 9598 5449 1010 5658 1997 17834 6666 1010 4359 2034 4284 6599 2011 3155 1002 2382 2454 4102 2000 1996 3188 2095 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   ******\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   unique_id: 3\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   tokens: [CLS] in the consumer - to - consumer segment , revenues were flat in the quarter or increased 2 % constant currency , while transactions grew 2 % [SEP]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_ids: 101 1999 1996 7325 1011 2000 1011 7325 6903 1010 12594 2020 4257 1999 1996 4284 2030 3445 1016 1003 5377 9598 1010 2096 11817 3473 1016 1003 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   ******\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   unique_id: 4\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   tokens: [CLS] c2 ##c constant currency revenue benefited from strong growth in western ##uni ##on [SEP]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_ids: 101 29248 2278 5377 9598 6599 19727 2013 2844 3930 1999 2530 19496 2239 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "Max sentence length: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUmsUOIv8EUO",
        "colab_type": "text"
      },
      "source": [
        "## Making Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQjig_Wrg9X-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For Prediction, we try higher batch size of 32\n",
        "\n",
        "batch_size = 32\n",
        "local_rank = -1 \n",
        "#local_rank for distributed training on gpus\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "unique_id_to_feature = {}\n",
        "for feature in features:\n",
        "    unique_id_to_feature[feature.unique_id] = feature\n",
        "\n",
        "#if local_rank != -1:\n",
        "    #model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
        "#elif n_gpu > 1:\n",
        "    #model = torch.nn.DataParallel(model)\n",
        "\n",
        "\n",
        "# Convert to tensors, need \"input_ids & its index\", \"input_mask\" and \"input_label\"\n",
        "# For testing set\n",
        "prediction_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long) # Token ids for every sentences in individual list\n",
        "prediction_input_ids_index = torch.arange(prediction_input_ids.size(0), dtype=torch.long) # Index for each sentences in one list\n",
        "prediction_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "\n",
        "prediction_data = TensorDataset(prediction_input_ids, prediction_input_mask, prediction_input_ids_index)\n",
        "\n",
        "# Create the DataLoader for our testing set.\n",
        "if local_rank == -1:\n",
        "    prediction_sampler = SequentialSampler(prediction_data)\n",
        "else:\n",
        "    prediction_sampler = DistributedSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size) \n",
        "# No of item in dataloader = Total sample / Batch_size\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhR99IISNMg9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab_type": "code",
        "outputId": "f917127f-021d-41b6-bc18-4a4cdf047791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions = None\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_input_ids_index = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits, s_logits, e_logits = model(b_input_ids, token_type_ids=None, \n",
        "                                         attention_mask=b_input_mask)\n",
        "\n",
        "      y_prob = logits.softmax(dim = -1) # normalizes values along axis 1\n",
        "      s_y_prob = s_logits.softmax(dim = -1)\n",
        "      e_y_prob = e_logits.softmax(dim = -1)\n",
        "\n",
        "      if predictions is None:\n",
        "        predictions = y_prob.detach().cpu().numpy()\n",
        "        \n",
        "        s_predictions = s_y_prob.detach().cpu().numpy()\n",
        "        s_class = np.argmax(s_predictions, axis=1).flatten()\n",
        "        \n",
        "        e_predictions = e_y_prob.detach().cpu().numpy()\n",
        "        e_class = np.argmax(e_predictions, axis=1).flatten()\n",
        "\n",
        "      else:\n",
        "        predictions = np.concatenate((predictions, y_prob.detach().cpu().numpy()), axis=0)\n",
        "\n",
        "        s_predictions = np.concatenate((s_predictions, s_y_prob.detach().cpu().numpy()), axis=0)\n",
        "        s_class = np.argmax(s_predictions, axis=1).flatten()\n",
        "        \n",
        "        e_predictions = np.concatenate((e_predictions, e_y_prob.detach().cpu().numpy()), axis=0)\n",
        "        e_class = np.argmax(e_predictions, axis=1).flatten()\n",
        "  \n",
        "print('    DONE.')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 213 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAqtQ3pcbU3T",
        "colab_type": "text"
      },
      "source": [
        "For binary classification:  \n",
        "y_prob = logits.sigmoid().cpu().numpy()   \n",
        "prediction = np.argmax(y_prob, 1)   \n",
        "\n",
        "For multi-label classification:  \n",
        "y_prob = logits.softmax(-1).cpu().detach().numpy()  \n",
        "prediction = np.argmax(y_prob, 1)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IOEYYuMkols",
        "colab_type": "code",
        "outputId": "ca4cc413-0ec5-4b72-f6cc-7e8eb241a3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "predictions = np.round(predictions, decimals=3)\n",
        "print(\"Sum of Aspect Mining Total Probability across \" + str(len(predictions[0])) + \" Aspects is \" + str(sum(predictions[0])))\n",
        "\n",
        "s_predictions = np.round(s_predictions, decimals=3)\n",
        "print(\"Sum of Sentiment Classification Total Probability across \" + str(len(s_predictions[0])) + \" Sentiment is \" + str(sum(s_predictions[0])))\n",
        "\n",
        "e_predictions = np.round(e_predictions, decimals=3)\n",
        "print(\"Sum of Emotion Classification Total Probability across \" + str(len(e_predictions[0])) + \" Emotion is \" + str(sum(e_predictions[0])))\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum of Aspect Mining Total Probability across 11 Aspects is 1.0000000069849193\n",
            "Sum of Sentiment Classification Total Probability across 3 Sentiment is 0.9999999919673428\n",
            "Sum of Emotion Classification Total Probability across 4 Emotion is 1.0000000015133992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6K4JCPRjJXZ",
        "colab_type": "code",
        "outputId": "a450214b-db79-4afb-e9e2-bbd9aeea0721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "# Extract the text used by the tokenizer\n",
        "flat_txt = []\n",
        "for (txt_index, sent_pair) in enumerate(corpus):\n",
        "  txt = sent_pair.text_a\n",
        "  flat_txt.append(txt)\n",
        "\n",
        "# Concat the ids to the Sentiments and Emotion lables\n",
        "s_txt = [str(s)+ \"-\" + Sid2label[int(s)] for s in np.nditer(s_class)]\n",
        "e_txt = [str(e)+ \"-\" + Eid2label[int(e)] for e in np.nditer(e_class)]\n",
        "\n",
        "# Concat the Predictions to a dataframe\n",
        "text_df = pd.DataFrame(data=flat_txt, columns = [\"text\"])\n",
        "s_class_df = pd.DataFrame(data=s_txt, columns = [\"Sentiment\"])\n",
        "e_class_df = pd.DataFrame(data=e_txt, columns = [\"Emotion\"])\n",
        "\n",
        "a_df = pd.DataFrame(data=predictions, columns = list(label2id))\n",
        "\n",
        "s_df = pd.DataFrame(data=s_predictions, columns = list(Slabel2id))\n",
        "\n",
        "e_df = pd.DataFrame(data=e_predictions, columns = list(Elabel2id))\n",
        "\n",
        "output_df = pd.concat([text_df, s_class_df, e_class_df, a_df, s_df, e_df], axis=1)\n",
        "\n",
        "# Saving to CSV\n",
        "pred_name = f'predicted_{file_name}.csv'\n",
        "output_df.to_csv(pred_name, index=True, header=True)\n",
        "\n",
        "output_df.sample(3)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>sales</th>\n",
              "      <th>earnings</th>\n",
              "      <th>op_costs</th>\n",
              "      <th>products_services</th>\n",
              "      <th>organic_expansion</th>\n",
              "      <th>acquisitions</th>\n",
              "      <th>competition</th>\n",
              "      <th>op_risks</th>\n",
              "      <th>debt</th>\n",
              "      <th>not_applicable</th>\n",
              "      <th>NIL</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Neutral</th>\n",
              "      <th>Positive</th>\n",
              "      <th>Confident</th>\n",
              "      <th>Dodgy</th>\n",
              "      <th>NIL</th>\n",
              "      <th>Uncertain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>That's something that we are extremely optimis...</td>\n",
              "      <td>2-Positive</td>\n",
              "      <td>0-Confident</td>\n",
              "      <td>0.632</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.953</td>\n",
              "      <td>0.956</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>The outstanding share count at quarter end was...</td>\n",
              "      <td>2-Positive</td>\n",
              "      <td>2-NIL</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.610</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.617</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>and State governments announced in January to ...</td>\n",
              "      <td>1-Neutral</td>\n",
              "      <td>2-NIL</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.958</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.941</td>\n",
              "      <td>0.011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  ... Uncertain\n",
              "209  That's something that we are extremely optimis...  ...     0.012\n",
              "79   The outstanding share count at quarter end was...  ...     0.003\n",
              "90   and State governments announced in January to ...  ...     0.011\n",
              "\n",
              "[3 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HlkK6fPer9Z",
        "colab_type": "code",
        "outputId": "ffdebda0-08be-41eb-bc98-d7bf31b46afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# copy files to Google Drive\n",
        "!zip -r /content/OutputBase_Multitask.zip /content/*.csv\n",
        "!cp -r /content/OutputBase_Multitask.zip \"./drive/My Drive/Downloads\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/predicted_Western Union Co_20170502-Text.csv (deflated 71%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}