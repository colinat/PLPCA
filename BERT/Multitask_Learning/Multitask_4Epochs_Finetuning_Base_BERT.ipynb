{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multitask_4Epochs_Finetuning_Base_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba289c320c264462968b78f1b5e09008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e87413bd03214f89b91fd1f65ed79aa6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3440b02234544b3190acfb5f57a85d75",
              "IPY_MODEL_f18d13db85b7457ba0ea28d3fa5ff6e6"
            ]
          }
        },
        "e87413bd03214f89b91fd1f65ed79aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3440b02234544b3190acfb5f57a85d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_837c96a3be0f4071a4ae142598b134c8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8b1bdefb88041309898fb4910ce467c"
          }
        },
        "f18d13db85b7457ba0ea28d3fa5ff6e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_14086ee084c247f9b7a005a5bfb5282f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 801kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe185468f6dc438ca7ddcbbeb32c699f"
          }
        },
        "837c96a3be0f4071a4ae142598b134c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8b1bdefb88041309898fb4910ce467c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14086ee084c247f9b7a005a5bfb5282f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe185468f6dc438ca7ddcbbeb32c699f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0d0d547997947cba85c9e5736fb4cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1cd6c8a116114385b6973045df4d90e7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4691ea44be4d4a08b5b2ecc7fff8c4a9",
              "IPY_MODEL_fbca06da4bfa48eb94e5a553332cef17"
            ]
          }
        },
        "1cd6c8a116114385b6973045df4d90e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4691ea44be4d4a08b5b2ecc7fff8c4a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_66e9a7b2439c4e51b5a46f9ad97589f0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73b45f6b2b57415d9a32091b45164fc2"
          }
        },
        "fbca06da4bfa48eb94e5a553332cef17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ae46c9f003aa45a2a3cbf48a95a3bf5f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 361/361 [00:01&lt;00:00, 298B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9c07302a78d4f5692bade83c8c918e4"
          }
        },
        "66e9a7b2439c4e51b5a46f9ad97589f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73b45f6b2b57415d9a32091b45164fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae46c9f003aa45a2a3cbf48a95a3bf5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9c07302a78d4f5692bade83c8c918e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5e902f2f597d430eb643ce4c2117e438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7550d5c250724615a036083645b94200",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_109e1349f1eb4e3fa5b2fb5d4e8a5d87",
              "IPY_MODEL_4a0abe932a1a4dd788e7676701f80f48"
            ]
          }
        },
        "7550d5c250724615a036083645b94200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "109e1349f1eb4e3fa5b2fb5d4e8a5d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d4342e3d8dd14dd983002a78172c07b9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bf220432a6f94db8a35124c3c2cab40a"
          }
        },
        "4a0abe932a1a4dd788e7676701f80f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a05c7dad4f8433c8389949ad5bd5ede",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:15&lt;00:00, 28.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d4004385f7e43168e1e89f065f0a453"
          }
        },
        "d4342e3d8dd14dd983002a78172c07b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bf220432a6f94db8a35124c3c2cab40a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a05c7dad4f8433c8389949ad5bd5ede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d4004385f7e43168e1e89f065f0a453": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSU7yERLP_66",
        "colab_type": "text"
      },
      "source": [
        "## Using Colab GPU for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eot6_cHkIxHu",
        "colab_type": "code",
        "outputId": "73d8edc5-a7e1-45ba-a449-d3b123cd069d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!rm -rf NLP_Colab\n",
        "!git clone https://github.com/sumkh/NLP_Colab.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP_Colab'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 81 (delta 44), reused 30 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (81/81), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjPyC2CtSUjV",
        "colab_type": "code",
        "outputId": "ad4d9e09-76e2-4664-ad54-af5f93ad21f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab_type": "code",
        "outputId": "d30db110-f2a6-4086-abcf-98c5ed1c3d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab_type": "code",
        "outputId": "91be0645-ef13-4640-9fbc-c54d2af23d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q",
        "colab_type": "code",
        "outputId": "88a183ec-0ee5-4cf3-c7b2-14df86bffa64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install transformers -q"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501kB 6.9MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 29.5MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 45.9MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.7MB 39.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amSbgl4B6u1j",
        "colab_type": "text"
      },
      "source": [
        "# Training and Fine-Tuning BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2qG0uKq6BMs",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Training and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ",
        "colab_type": "code",
        "outputId": "7fca3255-4832-4a55-87ff-147f807d2c76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"NLP_Colab/Multi.csv\")\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 5 random rows from the data.\n",
        "df.sample(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 6,697\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mydata</th>\n",
              "      <th>Aspects</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>sales</th>\n",
              "      <th>earnings</th>\n",
              "      <th>op_costs</th>\n",
              "      <th>products_services</th>\n",
              "      <th>organic_expansion</th>\n",
              "      <th>acquisitions</th>\n",
              "      <th>competition</th>\n",
              "      <th>op_risks</th>\n",
              "      <th>debt</th>\n",
              "      <th>not_applicable</th>\n",
              "      <th>NIL</th>\n",
              "      <th>Slabel_f</th>\n",
              "      <th>Elabel_f</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5662</th>\n",
              "      <td>We believe issuance growth will return to 3% r...</td>\n",
              "      <td>sales</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>NIL</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6272</th>\n",
              "      <td>I wouldn't comment on next year, up or down, b...</td>\n",
              "      <td>sales</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Confident</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5415</th>\n",
              "      <td>The acquired integrated imaging solutions line...</td>\n",
              "      <td>sales</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Confident</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5420</th>\n",
              "      <td>It's a very healthy business for us</td>\n",
              "      <td>not_applicable</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>NIL</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3642</th>\n",
              "      <td>Our leverage position remains comfortable with...</td>\n",
              "      <td>debt</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>NIL</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 mydata  ... Elabel_f\n",
              "5662  We believe issuance growth will return to 3% r...  ...        2\n",
              "6272  I wouldn't comment on next year, up or down, b...  ...        0\n",
              "5415  The acquired integrated imaging solutions line...  ...        0\n",
              "5420                It's a very healthy business for us  ...        2\n",
              "3642  Our leverage position remains comfortable with...  ...        2\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8kEDRvShcU5",
        "colab_type": "text"
      },
      "source": [
        "## BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A",
        "colab_type": "code",
        "outputId": "2905b5e6-d92e-4233-f5aa-7a281d76b272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "ba289c320c264462968b78f1b5e09008",
            "e87413bd03214f89b91fd1f65ed79aa6",
            "3440b02234544b3190acfb5f57a85d75",
            "f18d13db85b7457ba0ea28d3fa5ff6e6",
            "837c96a3be0f4071a4ae142598b134c8",
            "a8b1bdefb88041309898fb4910ce467c",
            "14086ee084c247f9b7a005a5bfb5282f",
            "fe185468f6dc438ca7ddcbbeb32c699f"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba289c320c264462968b78f1b5e09008",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_widâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SMZ5T5Imhlx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Let's extract the sentences and labels of our training set as numpy ndarrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhtYMsJumEq6",
        "colab_type": "code",
        "outputId": "807a15ee-9624-4306-d65c-07e17c6a869f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Categories of Aspects\n",
        "label_list = [\"sales\",\"earnings\",\"op_costs\",\"products_services\",\"organic_expansion\",\"acquisitions\",\"competition\",\"op_risks\",\"debt\",\"not_applicable\",\"NIL\"]\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "num_labels = len(label_list) # 11\n",
        "\n",
        "# Categories of Sentiment\n",
        "Slabel_list = [\"Negative\",\"Neutral\",\"Positive\",] # Follow order in Slabel_f\n",
        "Slabel2id = {label: i for i, label in enumerate(Slabel_list)}\n",
        "Sid2label = {i: label for i, label in enumerate(Slabel_list)}\n",
        "s_num_labels = len(Slabel_list) # 3\n",
        "\n",
        "# Categories of Emotion\n",
        "Elabel_list = [\"Confident\",\"Dodgy\",\"NIL\",\"Uncertain\"] # Follow order in Elabel_f\n",
        "Elabel2id = {label: i for i, label in enumerate(Elabel_list)}\n",
        "Eid2label = {i: label for i, label in enumerate(Elabel_list)}\n",
        "e_num_labels = len(Elabel_list) # 4\n",
        "\n",
        "id2label"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'sales',\n",
              " 1: 'earnings',\n",
              " 2: 'op_costs',\n",
              " 3: 'products_services',\n",
              " 4: 'organic_expansion',\n",
              " 5: 'acquisitions',\n",
              " 6: 'competition',\n",
              " 7: 'op_risks',\n",
              " 8: 'debt',\n",
              " 9: 'not_applicable',\n",
              " 10: 'NIL'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNh9Fz6vEDAw",
        "colab_type": "code",
        "outputId": "014da192-e122-4f95-b41c-4a1c80845860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Sid2label"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Negative', 1: 'Neutral', 2: 'Positive'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBx39NY1EBB4",
        "colab_type": "code",
        "outputId": "a0d0a7c0-d5b7-4729-bb27-6d130d4f9517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Eid2label"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Confident', 1: 'Dodgy', 2: 'NIL', 3: 'Uncertain'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUijVxTGodqz",
        "colab_type": "code",
        "outputId": "a85a3ab3-b742-4081-a1ae-c0927adb10c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Set labels for Aspect\n",
        "label_f = df[[\"sales\",\"earnings\",\"op_costs\",\"products_services\",\"organic_expansion\",\"acquisitions\",\"competition\",\"op_risks\",\"debt\",\"not_applicable\",\"NIL\"]]\n",
        "label_f = label_f.values.tolist()\n",
        "print(label_list)\n",
        "print(\"Number of Labels: \" + str(num_labels))\n",
        "label_f[0:3]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sales', 'earnings', 'op_costs', 'products_services', 'organic_expansion', 'acquisitions', 'competition', 'op_risks', 'debt', 'not_applicable', 'NIL']\n",
            "Number of Labels: 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuCKRQbqp2im",
        "colab_type": "code",
        "outputId": "d26ed0af-bea7-43e2-9886-7bf447482cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Set labels for Sentiment\n",
        "s_label_f = df[[\"Slabel_f\"]]\n",
        "s_label_f = s_label_f.values.tolist()\n",
        "\n",
        "# Set labels for Emotion\n",
        "e_label_f = df[[\"Elabel_f\"]]\n",
        "e_label_f = e_label_f.values.tolist()\n",
        "\n",
        "# Convert text to numpy arrays\n",
        "text = df.iloc[:,0]\n",
        "text = text.values.tolist()\n",
        "text[0:3]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yeah',\n",
              " 'One is more for the data center',\n",
              " 'Premiere Pro is the worlds leading video production solution, and continues to grow its footprint across every video segment']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VglOTCywFZ8",
        "colab_type": "code",
        "outputId": "5a1b6942-50c4-45d0-a820-8b4cb7b937b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import collections\n",
        "import logging\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InputExample(object):\n",
        "\n",
        "    def __init__(self, unique_id, text_a, text_b, labels, s_labels, e_labels):\n",
        "        self.unique_id = unique_id\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.labels = labels\n",
        "        self.s_labels = s_labels\n",
        "        self.e_labels = e_labels\n",
        "\n",
        "# Get the lists of sentences and their labels.\n",
        "input_file = zip(text, label_f, s_label_f, e_label_f)\n",
        "\n",
        "corpus = []\n",
        "unique_id = 0\n",
        "for txt, lab_f, s_lab_f, e_lab_f in input_file:\n",
        "  line = txt.strip()\n",
        "  text_a = None\n",
        "  text_b = None\n",
        "  m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
        "  if m is None:\n",
        "    text_a = line\n",
        "  else:\n",
        "    text_a = m.group(1)\n",
        "    text_b = m.group(2)\n",
        "  \n",
        "  label = [np.float(x) for x in lab_f]\n",
        "  \n",
        "  corpus.append(InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b, \n",
        "                             labels = label, s_labels = s_lab_f, e_labels = e_lab_f))\n",
        "  unique_id += 1\n",
        "\n",
        "MAX_LEN = int(math.ceil(max([len(keys.split()) for keys in text])/10)*10)\n",
        "print('Max sentence length: ' + str(MAX_LEN))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length: 160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOn_qMZj3QTg",
        "colab_type": "code",
        "outputId": "c6727850-f465-45c6-dc99-4cb47e1bef73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        }
      },
      "source": [
        "# Set the maximum sequence length.\n",
        "# In the original paper, the authors used a length of 512.\n",
        "seq_length = MAX_LEN \n",
        "# type=int\n",
        "# The maximum total input sequence length after WordPiece tokenization. \n",
        "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, unique_id, labels, s_labels, e_labels, tokens, input_ids, input_mask, input_type_ids):\n",
        "        self.unique_id = unique_id\n",
        "        self.labels = labels\n",
        "        self.s_labels = s_labels\n",
        "        self.e_labels = e_labels\n",
        "        self.tokens = tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.input_type_ids = input_type_ids\n",
        "\n",
        "# Reference: corpus.(unique_id, text_a, text_b, labels, s_labels, e_labels)\n",
        "features = []\n",
        "for (txt_index, sent_pair) in enumerate(corpus):\n",
        "    tokens_a = tokenizer.tokenize(sent_pair.text_a)\n",
        "\n",
        "    tokens_b = None\n",
        "    if sent_pair.text_b:\n",
        "        tokens_b = tokenizer.tokenize(sent_pair.text_b)\n",
        "\n",
        "    if tokens_b:\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "        # length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
        "    else:\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > seq_length - 2:\n",
        "            tokens_a = tokens_a[0:(seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    input_type_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    input_type_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        input_type_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    input_type_ids.append(0)\n",
        "\n",
        "    if tokens_b:\n",
        "        for token in tokens_b:\n",
        "            tokens.append(token)\n",
        "            input_type_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        input_type_ids.append(1)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        input_type_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == seq_length\n",
        "    assert len(input_mask) == seq_length\n",
        "    assert len(input_type_ids) == seq_length\n",
        "\n",
        "    if txt_index < 5:\n",
        "        logger.info(\"******\")\n",
        "        logger.info(\"unique_id: %s\" % (sent_pair.unique_id))\n",
        "        logger.info(\"Aspect labels: %s\" % (sent_pair.labels))\n",
        "        logger.info(\"Sentiment labels: %s\" % (sent_pair.s_labels))\n",
        "        logger.info(\"Emotion labels: %s\" % (sent_pair.e_labels))\n",
        "        logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "        logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "        logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "        logger.info(\n",
        "            \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
        "        \n",
        "    features.append(InputFeatures(\n",
        "                unique_id=sent_pair.unique_id,\n",
        "                labels=sent_pair.labels,\n",
        "                s_labels=sent_pair.s_labels,\n",
        "                e_labels=sent_pair.e_labels,\n",
        "                tokens=tokens,\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                input_type_ids=input_type_ids))\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 09:59:48 - INFO - __main__ -   ******\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   unique_id: 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Aspect labels: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Sentiment labels: [1]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Emotion labels: [2]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   tokens: [CLS] yeah [SEP]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_ids: 101 3398 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   ******\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   unique_id: 1\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Aspect labels: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Sentiment labels: [1]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Emotion labels: [2]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   tokens: [CLS] one is more for the data center [SEP]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_ids: 101 2028 2003 2062 2005 1996 2951 2415 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   ******\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   unique_id: 2\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Aspect labels: [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Sentiment labels: [2]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Emotion labels: [0]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   tokens: [CLS] premiere pro is the worlds leading video production solution , and continues to grow its footprint across every video segment [SEP]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_ids: 101 6765 4013 2003 1996 8484 2877 2678 2537 5576 1010 1998 4247 2000 4982 2049 24319 2408 2296 2678 6903 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   ******\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   unique_id: 3\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Aspect labels: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Sentiment labels: [1]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Emotion labels: [2]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   tokens: [CLS] if they ' re ds ##a , now they can buy imagine manager [SEP]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_ids: 101 2065 2027 1005 2128 16233 2050 1010 2085 2027 2064 4965 5674 3208 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   ******\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   unique_id: 4\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Aspect labels: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Sentiment labels: [2]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   Emotion labels: [0]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   tokens: [CLS] we also saw orders growth in all regions with particular strength in asia , which grew 26 % year - over - year , with strength in china [SEP]\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_ids: 101 2057 2036 2387 4449 3930 1999 2035 4655 2007 3327 3997 1999 4021 1010 2029 3473 2656 1003 2095 1011 2058 1011 2095 1010 2007 3997 1999 2859 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 09:59:48 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu0ao7p8rb06",
        "colab_type": "text"
      },
      "source": [
        "Divide up our training set to use 90% for training and 10% for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFbE-UHvsb7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "t_features, v_features = train_test_split(features, random_state=123, test_size=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LzSbTqW9_BR",
        "colab_type": "text"
      },
      "source": [
        "## Converting to PyTorch Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p1uXczp-Je4",
        "colab_type": "text"
      },
      "source": [
        "Our model expects PyTorch tensors rather than numpy.ndarrays, so convert all of our dataset variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD9i6Z2pG-sN",
        "colab_type": "text"
      },
      "source": [
        "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw5K2A5Ko1RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32. \n",
        "# For Colab, recommend batch size of 8\n",
        "\n",
        "batch_size = 8\n",
        "local_rank = -1 \n",
        "#local_rank for distributed training on gpus\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "unique_id_to_feature = {}\n",
        "for feature in t_features:\n",
        "    unique_id_to_feature[feature.unique_id] = feature\n",
        "\n",
        "if local_rank != -1:\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
        "elif n_gpu > 1:\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "\n",
        "# Convert to tensors, need \"input_ids & its index\", \"input_mask\" and \"input_label\"\n",
        "# For Training set\n",
        "t_input_ids = torch.tensor([f.input_ids for f in t_features], dtype=torch.long) # Token ids for every sentences in individual list\n",
        "t_input_ids_index = torch.arange(t_input_ids.size(0), dtype=torch.long) # Index for each sentences in one list\n",
        "t_input_mask = torch.tensor([f.input_mask for f in t_features], dtype=torch.long)\n",
        "\n",
        "# Training and Labels for each task\n",
        "t_input_label = torch.tensor([f.labels for f in t_features], dtype=torch.long)\n",
        "t_input_s_label = torch.tensor([f.s_labels for f in t_features], dtype=torch.long)\n",
        "t_input_e_label = torch.tensor([f.e_labels for f in t_features], dtype=torch.long)\n",
        "\n",
        "train_data = TensorDataset(t_input_ids, t_input_mask, \n",
        "                           t_input_label, t_input_s_label, t_input_e_label, \n",
        "                           t_input_ids_index)\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "if local_rank == -1:\n",
        "    train_sampler = SequentialSampler(train_data)\n",
        "else:\n",
        "    train_sampler = DistributedSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # No of item in dataloader = Total sample / Batch_size\n",
        "\n",
        "\n",
        "# For Validation set\n",
        "v_input_ids = torch.tensor([f.input_ids for f in v_features], dtype=torch.long) # Token ids for every sentences in individual list\n",
        "v_input_ids_index = torch.arange(v_input_ids.size(0), dtype=torch.long) # Index for each sentences in one list\n",
        "v_input_mask = torch.tensor([f.input_mask for f in v_features], dtype=torch.long)\n",
        "\n",
        "# Validation and Labels for each task\n",
        "v_input_label = torch.tensor([f.labels for f in v_features], dtype=torch.long)\n",
        "v_input_s_label = torch.tensor([f.s_labels for f in v_features], dtype=torch.long)\n",
        "v_input_e_label = torch.tensor([f.e_labels for f in v_features], dtype=torch.long)\n",
        "\n",
        "validation_data = TensorDataset(v_input_ids, v_input_mask, \n",
        "                                v_input_label, v_input_s_label, v_input_e_label,\n",
        "                                v_input_ids_index)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "if local_rank == -1:\n",
        "    validation_sampler = SequentialSampler(validation_data)\n",
        "else:\n",
        "    validation_sampler = DistributedSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size) # No of item in dataloader = Total sample / Batch_size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdldbRhq1gGj",
        "colab_type": "text"
      },
      "source": [
        "## Defining the Classification Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc9G4IqGzWQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from transformers.modeling_bert import BertPreTrainedModel, BertModel\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "class BertForMultitask(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertForMultitask, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        self.classifier = nn.Linear(config.hidden_size, out_features=11)\n",
        "        self.s_classifier = nn.Linear(config.hidden_size, out_features=3)\n",
        "        self.e_classifier = nn.Linear(config.hidden_size, out_features=4)\n",
        "\n",
        "        # define dropout layer in __init__\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None, \n",
        "                labels=None, s_labels=None, e_labels=None):\n",
        "        _, pooled_output  = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        s_logits = self.s_classifier(pooled_output)\n",
        "\n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        e_logits = self.e_classifier(pooled_output)\n",
        "        \n",
        "        # When training, provides label and loss will be included into output, \n",
        "        # for validation, no label will be entered\n",
        "        if labels is not None:\n",
        "          loss_fct = BCEWithLogitsLoss() ## pos_weight=[1,1,1,1,1,1,1,1,1,1,1]\n",
        "          loss = loss_fct(logits.float(), labels.float())\n",
        "        \n",
        "          s_loss_fct = CrossEntropyLoss()\n",
        "          s_loss = s_loss_fct(s_logits.view(-1, 3), s_labels.view(-1))\n",
        "          \n",
        "          e_loss_fct = CrossEntropyLoss()\n",
        "          e_loss = e_loss_fct(e_logits.view(-1, 4), e_labels.view(-1))\n",
        "        \n",
        "          # view function is meant to reshape the tensor. \n",
        "          # If there is any situation that you don't know how many rows you \n",
        "          # want but are sure of the number of columns (i.e. num_label), \n",
        "          # then you can specify this with a -1 so that the logits are reshaped\n",
        "          # to matrix of 1xNum_label\n",
        "        \n",
        "          losses = loss + s_loss + e_loss\n",
        "          return losses\n",
        "        \n",
        "        else: \n",
        "          all_logits = logits, s_logits, e_logits\n",
        "          return all_logits\n",
        "\n",
        "        outputs = (losses,) + all_logits\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def freeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def unfreeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDqUq7Eizva1",
        "colab_type": "text"
      },
      "source": [
        "### Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR4OqsNEr4Vw",
        "colab_type": "code",
        "outputId": "1c9e4bbe-56f2-4c6e-cbc6-0cdc14ef5d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f0d0d547997947cba85c9e5736fb4cb5",
            "1cd6c8a116114385b6973045df4d90e7",
            "4691ea44be4d4a08b5b2ecc7fff8c4a9",
            "fbca06da4bfa48eb94e5a553332cef17",
            "66e9a7b2439c4e51b5a46f9ad97589f0",
            "73b45f6b2b57415d9a32091b45164fc2",
            "ae46c9f003aa45a2a3cbf48a95a3bf5f",
            "b9c07302a78d4f5692bade83c8c918e4",
            "5e902f2f597d430eb643ce4c2117e438",
            "7550d5c250724615a036083645b94200",
            "109e1349f1eb4e3fa5b2fb5d4e8a5d87",
            "4a0abe932a1a4dd788e7676701f80f48",
            "d4342e3d8dd14dd983002a78172c07b9",
            "bf220432a6f94db8a35124c3c2cab40a",
            "0a05c7dad4f8433c8389949ad5bd5ede",
            "3d4004385f7e43168e1e89f065f0a453"
          ]
        }
      },
      "source": [
        "from transformers import AdamW, BertConfig\n",
        "\n",
        "# Load Custom layer for BertForMultitask, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForMultitask.from_pretrained(\"bert-base-uncased\")   \n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 10:00:22 - INFO - filelock -   Lock 139981280893976 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
            "03/24/2020 10:00:22 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpi9s_9_3l\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0d0d547997947cba85c9e5736fb4cb5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 10:00:22 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/24/2020 10:00:22 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/24/2020 10:00:22 - INFO - filelock -   Lock 139981280893976 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
            "03/24/2020 10:00:22 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/24/2020 10:00:22 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 10:00:23 - INFO - filelock -   Lock 139981262298472 acquired on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "03/24/2020 10:00:23 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpzqqf6ncz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e902f2f597d430eb643ce4c2117e438",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 10:00:39 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/24/2020 10:00:39 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/24/2020 10:00:39 - INFO - filelock -   Lock 139981262298472 released on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "03/24/2020 10:00:39 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 10:00:42 - INFO - transformers.modeling_utils -   Weights of BertForMultitask not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 's_classifier.weight', 's_classifier.bias', 'e_classifier.weight', 'e_classifier.bias']\n",
            "03/24/2020 10:00:42 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMultitask: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMultitask(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
              "  (s_classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              "  (e_classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u-0yImuPCZF",
        "colab_type": "text"
      },
      "source": [
        "### Model's Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Jv6c7-HHDW",
        "colab_type": "text"
      },
      "source": [
        "Just for curiosity's sake, we can browse all of the model's parameters by name here.\n",
        "\n",
        "In the below cell, I've printed out the names and dimensions of the weights for:\n",
        "\n",
        "1. The embedding layer.\n",
        "2. The first of the twelve transformers.\n",
        "3. The output layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIiVlDYCtSq",
        "colab_type": "code",
        "outputId": "26f2381d-ec4d-4703-a6ae-b5adf14b8ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-8:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 205 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                          (11, 768)\n",
            "classifier.bias                                                (11,)\n",
            "s_classifier.weight                                         (3, 768)\n",
            "s_classifier.bias                                               (3,)\n",
            "e_classifier.weight                                         (4, 768)\n",
            "e_classifier.bias                                               (4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWT-D4U_Pvx",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o-VEBobKwHk",
        "colab_type": "text"
      },
      "source": [
        "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
        "\n",
        "For the purposes of fine-tuning, the authors recommend choosing from the following values:\n",
        "- Batch size: 16, 32  (We chose **8** when creating our DataLoaders).\n",
        "- Learning rate (Adam): 5e-5, 3e-5, 2e-5  (**We'll use 2e-5**).\n",
        "- Number of epochs: 2, 3, 4  (**We'll use 4**).\n",
        "\n",
        "The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
        "\n",
        "You can find the creation of the AdamW optimizer in `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfmWwUR_Sox",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE5B99H5H2-W",
        "colab_type": "text"
      },
      "source": [
        "Define a helper function for calculating accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw3OaPY0sYD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Reference Compute Softmax function using numpy:\n",
        "# https://gist.github.com/alceufc/f3fd0cd7d9efb120195c\n",
        "def np_softmax(x):\n",
        "    scoreMatExp = np.exp(np.asarray(x))\n",
        "    return scoreMatExp / scoreMatExp.sum(0)\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    preds = np_softmax(preds)\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def accuracy_thresh(y_pred, y_true, thresh=0.5, sigmoid=True):\n",
        "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
        "    if sigmoid: y_pred = y_pred.sigmoid()\n",
        "    return np.mean(((y_pred>thresh)==y_true.byte()).float().cpu().numpy(), axis=1).sum()\n",
        "\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab_type": "code",
        "outputId": "07c102bd-1c24-4020-c683-69d2b4e940a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 123\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "eval_loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels (Label for Aspect Mining task)\n",
        "        #   [3]: s_label (Label for Sentiment Classification task)\n",
        "        #   [4]: e_label (Label for Emotion Classification task)\n",
        "        #   [5]: input_ids_index\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_s_labels = batch[3].to(device)\n",
        "        b_e_labels = batch[4].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask,\n",
        "                        labels=b_labels, \n",
        "                        s_labels=b_s_labels, \n",
        "                        e_labels=b_e_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs\n",
        "        \n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.mean().item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    all_logits = None\n",
        "    all_labels = None\n",
        "     \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    s_eval_loss, s_eval_accuracy = 0, 0\n",
        "    e_eval_loss, e_eval_accuracy = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels, b_s_labels, b_e_labels, b_input_ids_index = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            \n",
        "            #logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            logits, s_logits, e_logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax (here BCEWithLogitsLoss).\n",
        "            loss_fct = BCEWithLogitsLoss() # pos_weight=[1,1,1,1,1,1,1,1,1,1,1]\n",
        "            tmp_eval_loss = loss_fct(logits.float(), b_labels.float())\n",
        "            \n",
        "            # For Sentiment and Emotion Validation:  \n",
        "            s_loss_fct = CrossEntropyLoss()\n",
        "            tmp_s_eval_loss = s_loss_fct(s_logits.view(-1, 3), b_s_labels.view(-1))\n",
        "          \n",
        "            e_loss_fct = CrossEntropyLoss()\n",
        "            tmp_e_eval_loss = e_loss_fct(e_logits.view(-1, 4), b_e_labels.view(-1))\n",
        "        \n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            s_logits = s_logits.detach().cpu().numpy()\n",
        "            s_label_ids = b_s_labels.to('cpu').numpy()\n",
        "\n",
        "            e_logits = e_logits.detach().cpu().numpy()\n",
        "            e_label_ids = b_e_labels.to('cpu').numpy()\n",
        "        \n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            s_tmp_eval_accuracy = flat_accuracy(s_logits, s_label_ids)\n",
        "            e_tmp_eval_accuracy = flat_accuracy(e_logits, e_label_ids)\n",
        "        \n",
        "            # Accumulate the total accuracy.\n",
        "            s_eval_accuracy += s_tmp_eval_accuracy\n",
        "            e_eval_accuracy += e_tmp_eval_accuracy\n",
        "\n",
        "          \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = accuracy_thresh(logits, b_labels)\n",
        "        if all_logits is None:\n",
        "          all_logits = logits.detach().cpu().numpy()\n",
        "        else:\n",
        "          all_logits = np.concatenate((all_logits, logits.detach().cpu().numpy()), axis=0)\n",
        "            \n",
        "        if all_labels is None:\n",
        "          all_labels = b_labels.detach().cpu().numpy()\n",
        "        else:    \n",
        "          all_labels = np.concatenate((all_labels, b_labels.detach().cpu().numpy()), axis=0)\n",
        "        \n",
        "        # Average evaluation loss per batch\n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        s_eval_loss += tmp_s_eval_loss.mean().item()\n",
        "        e_eval_loss += tmp_e_eval_loss.mean().item()\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        nb_eval_examples += b_labels.size(0)\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps # total loss/no. of batches\n",
        "    eval_accuracy = eval_accuracy / nb_eval_examples # total accuracy of each aspect / no. of aspect (11 x no.of batches)\n",
        "\n",
        "    s_eval_loss = s_eval_loss / nb_eval_steps \n",
        "    e_eval_loss = e_eval_loss / nb_eval_steps\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    total_eval_loss = (eval_loss + s_eval_loss + e_eval_loss)/3\n",
        "\n",
        "    # Store the loss value for plotting the validation curve.\n",
        "    eval_loss_values.append(total_eval_loss)\n",
        "\n",
        "        \n",
        "    # ROC-AUC calcualation\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    \n",
        "    for i in range(num_labels):\n",
        "        fpr[i], tpr[i], _ = roc_curve(all_labels[:, i], all_logits[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        \n",
        "    # Compute micro-average ROC curve and ROC area\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels.ravel(), all_logits.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    result = {'eval_loss': eval_loss,\n",
        "              'eval_accuracy': eval_accuracy,\n",
        "              'roc_auc': roc_auc }\n",
        "    \n",
        "    print(\"Aspects Mining:\")\n",
        "    print(\"  Loss:   {0:.2f}\".format(eval_loss) + \"    Accuracy:   {0:.2f}\".format(eval_accuracy))\n",
        "    #print(\"  Validation Accuracy: {0:.2f}\".format(eval_accuracy))\n",
        "    print(\"  ROC AUC: \")\n",
        "    print([str(i) + \":\" + str(int(roc_auc[i]*1000)/10)+\"%\" for i in roc_auc])\n",
        "    print(\"Sentiment Classification:\")\n",
        "    print(\"  Loss:   {0:.2f}\".format(s_eval_loss) + \"    Accuracy:   {0:.2f}\".format(s_eval_accuracy/nb_eval_steps))\n",
        "    #print(\"  Validation Accuracy: {0:.2f}\".format(s_eval_accuracy/nb_eval_steps))\n",
        "    print(\"Emotion Classification:\")\n",
        "    print(\"  Loss:   {0:.2f}\".format(e_eval_loss) + \"    Accuracy:   {0:.2f}\".format(e_eval_accuracy/nb_eval_steps))\n",
        "    #print(\"  Validation Accuracy: {0:.2f}\".format(e_eval_accuracy/nb_eval_steps))\n",
        "    print(\"\")\n",
        "    print(\"  Average Validation Loss: {0:.2f}\".format(total_eval_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:14.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:36.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:43.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:50.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:57.\n",
            "  Batch   360  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:25.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:32.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:39.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:46.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:54.\n",
            "  Batch   680  of    754.    Elapsed: 0:02:01.\n",
            "  Batch   720  of    754.    Elapsed: 0:02:08.\n",
            "\n",
            "  Average training loss: 1.50\n",
            "  Training epcoh took: 0:02:14\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.22    Accuracy:   0.93\n",
            "  ROC AUC: \n",
            "['0:89.3%', '1:73.1%', '2:64.1%', '3:69.6%', '4:68.8%', '5:63.3%', '6:60.8%', '7:41.6%', '8:49.0%', '9:96.0%', '10:41.5%', 'micro:88.0%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.44    Accuracy:   0.73\n",
            "Emotion Classification:\n",
            "  Loss:   0.56    Accuracy:   0.71\n",
            "\n",
            "  Average Validation Loss: 0.41\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:14.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:21.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:28.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:35.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:42.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:50.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:57.\n",
            "  Batch   360  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:25.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:32.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:39.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:46.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:53.\n",
            "  Batch   680  of    754.    Elapsed: 0:02:00.\n",
            "  Batch   720  of    754.    Elapsed: 0:02:08.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epcoh took: 0:02:13\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.21    Accuracy:   0.93\n",
            "  ROC AUC: \n",
            "['0:88.9%', '1:76.4%', '2:72.9%', '3:70.7%', '4:70.3%', '5:68.6%', '6:57.7%', '7:56.2%', '8:63.5%', '9:96.5%', '10:62.0%', 'micro:89.2%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.45    Accuracy:   0.75\n",
            "Emotion Classification:\n",
            "  Loss:   0.54    Accuracy:   0.69\n",
            "\n",
            "  Average Validation Loss: 0.40\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:14.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:21.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:28.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:35.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:42.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:50.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:57.\n",
            "  Batch   360  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:25.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:32.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:39.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:46.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:53.\n",
            "  Batch   680  of    754.    Elapsed: 0:02:00.\n",
            "  Batch   720  of    754.    Elapsed: 0:02:08.\n",
            "\n",
            "  Average training loss: 0.83\n",
            "  Training epcoh took: 0:02:14\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.21    Accuracy:   0.93\n",
            "  ROC AUC: \n",
            "['0:88.5%', '1:78.6%', '2:82.4%', '3:70.8%', '4:71.3%', '5:73.4%', '6:48.8%', '7:70.0%', '8:84.9%', '9:96.7%', '10:65.4%', 'micro:89.8%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.52    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   0.59    Accuracy:   0.69\n",
            "\n",
            "  Average Validation Loss: 0.44\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    754.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    754.    Elapsed: 0:00:14.\n",
            "  Batch   120  of    754.    Elapsed: 0:00:21.\n",
            "  Batch   160  of    754.    Elapsed: 0:00:28.\n",
            "  Batch   200  of    754.    Elapsed: 0:00:35.\n",
            "  Batch   240  of    754.    Elapsed: 0:00:43.\n",
            "  Batch   280  of    754.    Elapsed: 0:00:50.\n",
            "  Batch   320  of    754.    Elapsed: 0:00:57.\n",
            "  Batch   360  of    754.    Elapsed: 0:01:04.\n",
            "  Batch   400  of    754.    Elapsed: 0:01:11.\n",
            "  Batch   440  of    754.    Elapsed: 0:01:18.\n",
            "  Batch   480  of    754.    Elapsed: 0:01:25.\n",
            "  Batch   520  of    754.    Elapsed: 0:01:32.\n",
            "  Batch   560  of    754.    Elapsed: 0:01:39.\n",
            "  Batch   600  of    754.    Elapsed: 0:01:46.\n",
            "  Batch   640  of    754.    Elapsed: 0:01:53.\n",
            "  Batch   680  of    754.    Elapsed: 0:02:01.\n",
            "  Batch   720  of    754.    Elapsed: 0:02:08.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epcoh took: 0:02:14\n",
            "\n",
            "Running Validation...\n",
            "Aspects Mining:\n",
            "  Loss:   0.20    Accuracy:   0.93\n",
            "  ROC AUC: \n",
            "['0:88.9%', '1:79.4%', '2:81.6%', '3:72.0%', '4:70.8%', '5:73.7%', '6:55.9%', '7:81.0%', '8:85.8%', '9:96.6%', '10:68.9%', 'micro:90.4%']\n",
            "Sentiment Classification:\n",
            "  Loss:   0.51    Accuracy:   0.74\n",
            "Emotion Classification:\n",
            "  Loss:   0.61    Accuracy:   0.69\n",
            "\n",
            "  Average Validation Loss: 0.44\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab_type": "code",
        "outputId": "61d05575-7950-4406-9af3-26664f21450f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'r-o')\n",
        "plt.plot(eval_loss_values, 'g-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss [Red] vs Validation loss [Green]\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAGaCAYAAACopj13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRDJjKCciIiIikhlDORERERGRzP4/1+zGuDylzvEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2079Qyn8Mt8",
        "colab_type": "text"
      },
      "source": [
        "# Saving Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYErKiOw4q__",
        "colab_type": "text"
      },
      "source": [
        "This first cell (taken from `run_glue.py` [here](https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495)) writes the model and tokenizer out to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulTWaOr8QNY",
        "colab_type": "code",
        "outputId": "f3fded86-41ce-419c-88c2-9f40d52f3212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 10:12:15 - INFO - transformers.configuration_utils -   Configuration saved in ./model_save/config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 10:12:16 - INFO - transformers.modeling_utils -   Model weights saved in ./model_save/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-tjHkR7lc1I",
        "colab_type": "text"
      },
      "source": [
        "Let's check out the file sizes, out of curiosity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqMzI3VTCZo5",
        "colab_type": "code",
        "outputId": "3253edd3-1c6c-4cfe-d713-40cf54fb5625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 428012K\n",
            "-rw-r--r-- 1 root root      1K Mar 24 10:12 config.json\n",
            "-rw-r--r-- 1 root root 427767K Mar 24 10:12 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root      1K Mar 24 10:12 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1K Mar 24 10:12 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    227K Mar 24 10:12 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr_bt2rFlgDn",
        "colab_type": "text"
      },
      "source": [
        "The largest file is the model weights, at around 418 megabytes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUFUIQ8Cu8D",
        "colab_type": "code",
        "outputId": "7a6f9c6d-6084-44b7-debe-a5d4149a1159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 418M Mar 24 10:12 ./model_save/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzGKvOFAll_e",
        "colab_type": "text"
      },
      "source": [
        "To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxlZsafTC-V5",
        "colab_type": "code",
        "outputId": "45ead516-78a2-425a-cdaf-6bf559ae6956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Zip up the folder and copy zip file to Google Drive\n",
        "!zip -r /content/modelBase_Multitask.zip /content/model_save/\n",
        "!cp -r /content/modelBase_Multitask.zip \"./drive/My Drive/Downloads\""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/model_save/ (stored 0%)\n",
            "  adding: content/model_save/tokenizer_config.json (stored 0%)\n",
            "  adding: content/model_save/config.json (deflated 56%)\n",
            "  adding: content/model_save/special_tokens_map.json (deflated 40%)\n",
            "  adding: content/model_save/vocab.txt (deflated 53%)\n",
            "  adding: content/model_save/pytorch_model.bin (deflated 7%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypk0s-xs69Ha",
        "colab_type": "text"
      },
      "source": [
        "# Loading Saved Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0vstijw85SZ",
        "colab_type": "text"
      },
      "source": [
        "The following functions will load the model back from disk.  \n",
        "First load the classification layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "14CQo7-RAKRC",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from transformers.modeling_bert import BertPreTrainedModel, BertModel\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "class BertForMultitask(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertForMultitask, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        self.classifier = nn.Linear(config.hidden_size, out_features=11)\n",
        "        self.s_classifier = nn.Linear(config.hidden_size, out_features=3)\n",
        "        self.e_classifier = nn.Linear(config.hidden_size, out_features=4)\n",
        "\n",
        "        # define dropout layer in __init__\n",
        "        #self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None):\n",
        "        _, pooled_output  = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        #pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        #pooled_output = self.dropout(pooled_output)\n",
        "        s_logits = self.s_classifier(pooled_output)\n",
        "\n",
        "        # apply model dropout to each classifier layer, responseive to eval()\n",
        "        #pooled_output = self.dropout(pooled_output)\n",
        "        e_logits = self.e_classifier(pooled_output)\n",
        "        \n",
        "        outputs = logits, s_logits, e_logits\n",
        "        return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nskPzUM084zL",
        "colab_type": "code",
        "outputId": "2375a1d3-fc80-444e-8fa6-42a8599b97a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#output_dir = './drive/My Drive/EBAC_G/NLP_Project/Embeddings/model_Multitask/'\n",
        "output_dir = '/content/model_save/'\n",
        "\n",
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = BertForMultitask.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/24/2020 08:46:00 - INFO - transformers.configuration_utils -   loading configuration file /content/model_save/config.json\n",
            "03/24/2020 08:46:00 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMultitask\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/24/2020 08:46:00 - INFO - transformers.modeling_utils -   loading weights file /content/model_save/pytorch_model.bin\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   Model name '/content/model_save/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/content/model_save/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   Didn't find file /content/model_save/added_tokens.json. We won't load it.\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   loading file /content/model_save/vocab.txt\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   loading file /content/model_save/special_tokens_map.json\n",
            "03/24/2020 08:46:02 - INFO - transformers.tokenization_utils -   loading file /content/model_save/tokenizer_config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMultitask(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
              "  (s_classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              "  (e_classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3",
        "colab_type": "text"
      },
      "source": [
        "# Performance of Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWe0_JW21MyV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xoy5f3baPXx",
        "colab_type": "code",
        "outputId": "d872cb47-3a5c-47c0-a17e-8b373cd8c55d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import spacy\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoROEEJBN5bE",
        "colab_type": "code",
        "outputId": "4ada6871-1761-4972-ab9b-ce3a356d2666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "file_path = \"drive/My Drive/EBAC_G/NLP_Project/Embeddings/Western Union Co_20170502-Text.txt\"\n",
        "file_name = os.path.basename(file_path)\n",
        "file_name = os.path.splitext(file_name)[0]\n",
        "\n",
        "scripts = []\n",
        "with open(file_path, 'r') as file:\n",
        "  mydata = file.readlines()\n",
        "  for lines in mydata:\n",
        "    scripts.append(lines)\n",
        "\n",
        "# get sentence segemented review with #sentences > 2\n",
        "def sentence_segment_filter_docs(doc_array):\n",
        "    sentences = []\n",
        "    for doc in nlp.pipe(doc_array, disable=['parser', 'tagger', 'ner'], batch_size=1000, n_threads=8):\n",
        "        sentences.append([sent.text.strip() for sent in doc.sents])\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "print(f'Found {len(scripts)} transcripts')\n",
        "print(f'Tokenizing Transcripts...')\n",
        "\n",
        "sentences = sentence_segment_filter_docs(scripts)\n",
        "nr_sents = sum([len(s) for s in sentences])\n",
        "print(f'Segmented {nr_sents} transcript sentences')\n",
        "\n",
        "\n",
        "sentences = sentence_segment_filter_docs(scripts)\n",
        "\n",
        "# Save to file\n",
        "fn_out = f'corpus_{file_name}.txt'\n",
        "\n",
        "with open(fn_out, \"w\") as f:\n",
        "    for sents in tqdm(sentences):\n",
        "        real_sents = []\n",
        "        for s in sents:\n",
        "            x = s.replace(' ', '').replace('\\n', '')\n",
        "            if x != '':\n",
        "                real_sents.append(s.replace('\\n', ''))\n",
        "        # filter only paragraph more than or equal to 1 sentence        \n",
        "        if len(real_sents) >= 1:\n",
        "            str_to_write = \"\\n\".join(real_sents) + \"|||\" + \"\\n\"\n",
        "            f.write(str_to_write)\n",
        "\n",
        "print(f'Done writing to {fn_out}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 205 transcripts\n",
            "Tokenizing Transcripts...\n",
            "Segmented 213 transcript sentences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [00:00<00:00, 68775.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done writing to corpus_Western Union Co_20170502-Text.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7cpjyGni2_O",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing the Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0q7Y3vog7wM",
        "colab_type": "code",
        "outputId": "67a378c2-738f-4d8e-f16c-483dbe44f272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "import argparse\n",
        "import collections\n",
        "import logging\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InputExample(object):\n",
        "\n",
        "    def __init__(self, unique_id, text_a, text_b):\n",
        "        self.unique_id = unique_id\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "\n",
        "corpus = []\n",
        "unique_id = 0\n",
        "count = []\n",
        "with open(fn_out, \"r\", encoding='utf-8') as input_file:\n",
        "  for line in tqdm(input_file):\n",
        "    line = line.strip()\n",
        "    text_a = None\n",
        "    text_b = None\n",
        "    m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
        "    if m is None:\n",
        "      text_a = re.sub(r\"(\\|\\|\\|)$\", \"\", line)\n",
        "    else:\n",
        "      text_a = m.group(1)\n",
        "      text_b = m.group(2)\n",
        "    corpus.append(InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
        "    unique_id += 1\n",
        "    cnt = len(line.split())\n",
        "    count.append(cnt)\n",
        "\n",
        "MAX_LEN = int(math.ceil(max(count)/10)*10)\n",
        "print(' ')\n",
        "print('Max sentence length: ' + str(MAX_LEN))\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# In the original paper, the authors used a length of 512.\n",
        "seq_length = MAX_LEN \n",
        "# type=int\n",
        "# The maximum total input sequence length after WordPiece tokenization. \n",
        "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
        "        self.unique_id = unique_id\n",
        "        self.tokens = tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.input_type_ids = input_type_ids\n",
        "\n",
        "features = []\n",
        "for (txt_index, sent_pair) in enumerate(corpus):\n",
        "    tokens_a = tokenizer.tokenize(sent_pair.text_a)\n",
        "\n",
        "    tokens_b = None\n",
        "    if sent_pair.text_b:\n",
        "        tokens_b = tokenizer.tokenize(sent_pair.text_b)\n",
        "\n",
        "    if tokens_b:\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "        # length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
        "    else:\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > seq_length - 2:\n",
        "            tokens_a = tokens_a[0:(seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    input_type_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    input_type_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        input_type_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    input_type_ids.append(0)\n",
        "\n",
        "    if tokens_b:\n",
        "        for token in tokens_b:\n",
        "            tokens.append(token)\n",
        "            input_type_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        input_type_ids.append(1)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        input_type_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == seq_length\n",
        "    assert len(input_mask) == seq_length\n",
        "    assert len(input_type_ids) == seq_length\n",
        "\n",
        "    if txt_index < 5:\n",
        "        logger.info(\"******\")\n",
        "        logger.info(\"unique_id: %s\" % (sent_pair.unique_id))\n",
        "        logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "        logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "        logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "        logger.info(\"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
        "    \n",
        "        \n",
        "    features.append(InputFeatures(\n",
        "                unique_id=sent_pair.unique_id,\n",
        "                tokens=tokens,\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                input_type_ids=input_type_ids))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "213it [00:00, 56954.40it/s]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   ******\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   unique_id: 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   tokens: [CLS] thank you , hi ##km ##et [SEP]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_ids: 101 4067 2017 1010 7632 22287 3388 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   ******\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   unique_id: 1\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   tokens: [CLS] first quarter reported revenues of $ 1 . 3 billion were flat or increased 3 % on a constant currency basis compared to the prior - year period [SEP]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_ids: 101 2034 4284 2988 12594 1997 1002 1015 1012 1017 4551 2020 4257 2030 3445 1017 1003 2006 1037 5377 9598 3978 4102 2000 1996 3188 1011 2095 2558 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   ******\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   unique_id: 2\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   tokens: [CLS] the impact of currency translation , net of hedge benefits , reduced first quarter revenue by approximately $ 30 million compared to the prior year [SEP]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_ids: 101 1996 4254 1997 9598 5449 1010 5658 1997 17834 6666 1010 4359 2034 4284 6599 2011 3155 1002 2382 2454 4102 2000 1996 3188 2095 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   ******\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   unique_id: 3\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   tokens: [CLS] in the consumer - to - consumer segment , revenues were flat in the quarter or increased 2 % constant currency , while transactions grew 2 % [SEP]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_ids: 101 1999 1996 7325 1011 2000 1011 7325 6903 1010 12594 2020 4257 1999 1996 4284 2030 3445 1016 1003 5377 9598 1010 2096 11817 3473 1016 1003 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   ******\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   unique_id: 4\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   tokens: [CLS] c2 ##c constant currency revenue benefited from strong growth in western ##uni ##on [SEP]\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_ids: 101 29248 2278 5377 9598 6599 19727 2013 2844 3930 1999 2530 19496 2239 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24/2020 08:47:50 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "Max sentence length: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUmsUOIv8EUO",
        "colab_type": "text"
      },
      "source": [
        "## Making Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQjig_Wrg9X-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For Prediction, we try higher batch size of 32\n",
        "\n",
        "batch_size = 32\n",
        "local_rank = -1 \n",
        "#local_rank for distributed training on gpus\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "unique_id_to_feature = {}\n",
        "for feature in features:\n",
        "    unique_id_to_feature[feature.unique_id] = feature\n",
        "\n",
        "#if local_rank != -1:\n",
        "    #model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
        "#elif n_gpu > 1:\n",
        "    #model = torch.nn.DataParallel(model)\n",
        "\n",
        "\n",
        "# Convert to tensors, need \"input_ids & its index\", \"input_mask\" and \"input_label\"\n",
        "# For testing set\n",
        "prediction_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long) # Token ids for every sentences in individual list\n",
        "prediction_input_ids_index = torch.arange(prediction_input_ids.size(0), dtype=torch.long) # Index for each sentences in one list\n",
        "prediction_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "\n",
        "prediction_data = TensorDataset(prediction_input_ids, prediction_input_mask, prediction_input_ids_index)\n",
        "\n",
        "# Create the DataLoader for our testing set.\n",
        "if local_rank == -1:\n",
        "    prediction_sampler = SequentialSampler(prediction_data)\n",
        "else:\n",
        "    prediction_sampler = DistributedSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size) \n",
        "# No of item in dataloader = Total sample / Batch_size\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhR99IISNMg9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab_type": "code",
        "outputId": "f917127f-021d-41b6-bc18-4a4cdf047791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions = None\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_input_ids_index = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits, s_logits, e_logits = model(b_input_ids, token_type_ids=None, \n",
        "                                         attention_mask=b_input_mask)\n",
        "\n",
        "      y_prob = logits.softmax(dim = -1) # normalizes values along axis 1\n",
        "      s_y_prob = s_logits.softmax(dim = -1)\n",
        "      e_y_prob = e_logits.softmax(dim = -1)\n",
        "\n",
        "      if predictions is None:\n",
        "        predictions = y_prob.detach().cpu().numpy()\n",
        "        \n",
        "        s_predictions = s_y_prob.detach().cpu().numpy()\n",
        "        s_class = np.argmax(s_predictions, axis=1).flatten()\n",
        "        \n",
        "        e_predictions = e_y_prob.detach().cpu().numpy()\n",
        "        e_class = np.argmax(e_predictions, axis=1).flatten()\n",
        "\n",
        "      else:\n",
        "        predictions = np.concatenate((predictions, y_prob.detach().cpu().numpy()), axis=0)\n",
        "\n",
        "        s_predictions = np.concatenate((s_predictions, s_y_prob.detach().cpu().numpy()), axis=0)\n",
        "        s_class = np.argmax(s_predictions, axis=1).flatten()\n",
        "        \n",
        "        e_predictions = np.concatenate((e_predictions, e_y_prob.detach().cpu().numpy()), axis=0)\n",
        "        e_class = np.argmax(e_predictions, axis=1).flatten()\n",
        "  \n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 213 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAqtQ3pcbU3T",
        "colab_type": "text"
      },
      "source": [
        "For binary classification:  \n",
        "y_prob = logits.sigmoid().cpu().numpy()   \n",
        "prediction = np.argmax(y_prob, 1)   \n",
        "\n",
        "For multi-label classification:  \n",
        "y_prob = logits.softmax(-1).cpu().detach().numpy()  \n",
        "prediction = np.argmax(y_prob, 1)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IOEYYuMkols",
        "colab_type": "code",
        "outputId": "ca4cc413-0ec5-4b72-f6cc-7e8eb241a3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "predictions = np.round(predictions, decimals=3)\n",
        "print(\"Sum of Aspect Mining Total Probability across \" + str(len(predictions[0])) + \" Aspects is \" + str(sum(predictions[0])))\n",
        "\n",
        "s_predictions = np.round(s_predictions, decimals=3)\n",
        "print(\"Sum of Sentiment Classification Total Probability across \" + str(len(s_predictions[0])) + \" Sentiment is \" + str(sum(s_predictions[0])))\n",
        "\n",
        "e_predictions = np.round(e_predictions, decimals=3)\n",
        "print(\"Sum of Emotion Classification Total Probability across \" + str(len(e_predictions[0])) + \" Emotion is \" + str(sum(e_predictions[0])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum of Aspect Mining Total Probability across 11 Aspects is 1.0000000069849193\n",
            "Sum of Sentiment Classification Total Probability across 3 Sentiment is 0.9999999919673428\n",
            "Sum of Emotion Classification Total Probability across 4 Emotion is 1.0000000015133992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6K4JCPRjJXZ",
        "colab_type": "code",
        "outputId": "a450214b-db79-4afb-e9e2-bbd9aeea0721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "# Extract the text used by the tokenizer\n",
        "flat_txt = []\n",
        "for (txt_index, sent_pair) in enumerate(corpus):\n",
        "  txt = sent_pair.text_a\n",
        "  flat_txt.append(txt)\n",
        "\n",
        "# Concat the ids to the Sentiments and Emotion lables\n",
        "s_txt = [str(s)+ \"-\" + Sid2label[int(s)] for s in np.nditer(s_class)]\n",
        "e_txt = [str(e)+ \"-\" + Eid2label[int(e)] for e in np.nditer(e_class)]\n",
        "\n",
        "# Concat the Predictions to a dataframe\n",
        "text_df = pd.DataFrame(data=flat_txt, columns = [\"text\"])\n",
        "s_class_df = pd.DataFrame(data=s_txt, columns = [\"Sentiment\"])\n",
        "e_class_df = pd.DataFrame(data=e_txt, columns = [\"Emotion\"])\n",
        "\n",
        "a_df = pd.DataFrame(data=predictions, columns = list(label2id))\n",
        "\n",
        "s_df = pd.DataFrame(data=s_predictions, columns = list(Slabel2id))\n",
        "\n",
        "e_df = pd.DataFrame(data=e_predictions, columns = list(Elabel2id))\n",
        "\n",
        "output_df = pd.concat([text_df, s_class_df, e_class_df, a_df, s_df, e_df], axis=1)\n",
        "\n",
        "# Saving to CSV\n",
        "pred_name = f'predicted_{file_name}.csv'\n",
        "output_df.to_csv(pred_name, index=True, header=True)\n",
        "\n",
        "output_df.sample(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>sales</th>\n",
              "      <th>earnings</th>\n",
              "      <th>op_costs</th>\n",
              "      <th>products_services</th>\n",
              "      <th>organic_expansion</th>\n",
              "      <th>acquisitions</th>\n",
              "      <th>competition</th>\n",
              "      <th>op_risks</th>\n",
              "      <th>debt</th>\n",
              "      <th>not_applicable</th>\n",
              "      <th>NIL</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Neutral</th>\n",
              "      <th>Positive</th>\n",
              "      <th>Confident</th>\n",
              "      <th>Dodgy</th>\n",
              "      <th>NIL</th>\n",
              "      <th>Uncertain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>That's something that we are extremely optimis...</td>\n",
              "      <td>2-Positive</td>\n",
              "      <td>0-Confident</td>\n",
              "      <td>0.632</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.953</td>\n",
              "      <td>0.956</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>The outstanding share count at quarter end was...</td>\n",
              "      <td>2-Positive</td>\n",
              "      <td>2-NIL</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.610</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.617</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>and State governments announced in January to ...</td>\n",
              "      <td>1-Neutral</td>\n",
              "      <td>2-NIL</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.958</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.941</td>\n",
              "      <td>0.011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  ... Uncertain\n",
              "209  That's something that we are extremely optimis...  ...     0.012\n",
              "79   The outstanding share count at quarter end was...  ...     0.003\n",
              "90   and State governments announced in January to ...  ...     0.011\n",
              "\n",
              "[3 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HlkK6fPer9Z",
        "colab_type": "code",
        "outputId": "ffdebda0-08be-41eb-bc98-d7bf31b46afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# copy files to Google Drive\n",
        "!zip -r /content/OutputBase_Multitask.zip /content/*.csv\n",
        "!cp -r /content/OutputBase_Multitask.zip \"./drive/My Drive/Downloads\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/predicted_Western Union Co_20170502-Text.csv (deflated 71%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}