{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "sector = [\"Consumer_discretionaries\", \"Consumer_staples\", \"healthcare\",\n",
    "         \"Industrials\", \"InfoTech\"]\n",
    "\n",
    "file = []\n",
    "\n",
    "for sec in sector:\n",
    "    item = glob.glob(os.path.join(os.getcwd(), sec, \"*.txt\"))\n",
    "    file.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n"
     ]
    }
   ],
   "source": [
    "file_list = [item for sublist in file for item in sublist]\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/briansum/Google Drive/EBAC_G/NLP_Project/By Sector/Consumer_discretionaries/Target Corp._20170816-Text.txt', '/Users/briansum/Google Drive/EBAC_G/NLP_Project/By Sector/Consumer_discretionaries/Darden Restaurants_20171219-Text.txt']\n"
     ]
    }
   ],
   "source": [
    "print(file_list[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Target Corp._20170816-Text', 'Darden Restaurants_20171219-Text', 'Home Depot_20170221-Text']\n"
     ]
    }
   ],
   "source": [
    "filename_list = []\n",
    "\n",
    "for file_path in file_list:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_name = os.path.splitext(file_name)[0]\n",
    "    filename_list.append(file_name)\n",
    "    \n",
    "print(filename_list[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = []\n",
    "for file_path in file_list:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_name = os.path.splitext(file_name)[0]\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        mydata = file.readlines()\n",
    "        for lines in mydata:\n",
    "            scripts.append(lines)\n",
    "\n",
    "#Save the Combined Scripts to Disk\n",
    "with open(\"scripts.txt\", 'w') as output:\n",
    "    for row in scripts:\n",
    "        output.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thanks, John\\n',\n",
       " 'As you have been hearing from many of the industry peers, this continues to be a challenging competitive and consumer environment\\n',\n",
       " 'That’s why we are particularly pleased by the ongoing progress we saw in the second quarter when we gained further momentum in the areas that we are already performing well and so improvement in the areas where performance needed more focus and our aggressive has come from both stores and digital channels, wherever our guests wants to shop us\\n']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentence segemented review with #sentences > 2\n",
    "def sentence_segment_filter_docs(doc_array):\n",
    "    sentences = []\n",
    "\n",
    "    for doc in nlp.pipe(doc_array, disable=['parser', 'tagger', 'ner'], batch_size=1000, n_threads=8):\n",
    "        sentences.append([sent.text.strip() for sent in doc.sents])\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56244 transcripts\n",
      "Tokenizing Transcripts...\n",
      "Segmented 59408 transcript sentences\n"
     ]
    }
   ],
   "source": [
    "print(f'Found {len(scripts)} transcripts')\n",
    "print(f'Tokenizing Transcripts...')\n",
    "\n",
    "sentences = sentence_segment_filter_docs(scripts)\n",
    "nr_sents = sum([len(s) for s in sentences])\n",
    "print(f'Segmented {nr_sents} transcript sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "fn_out = f'transcript_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56244/56244 [00:00<00:00, 200795.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing to transcript_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(fn_out, \"w\") as f:\n",
    "    for sents in tqdm(sentences):\n",
    "        real_sents = []\n",
    "        for s in sents:\n",
    "            x = s.replace(' ', '').replace('\\n', '')\n",
    "            if x != '':\n",
    "                real_sents.append(s.replace('\\n', ''))\n",
    "        # filter only paragraph more than or equal to 1 sentence        \n",
    "        if len(real_sents) >= 1:\n",
    "            str_to_write = \"\\n\" + \"\\n\".join(real_sents) + \"\\n\"\n",
    "            f.write(str_to_write)\n",
    "\n",
    "print(f'Done writing to {fn_out}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
